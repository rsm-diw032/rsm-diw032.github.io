[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Dingran Wang",
    "section": "",
    "text": "Heyyy!"
  },
  {
    "objectID": "Resume.html",
    "href": "Resume.html",
    "title": "Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "My projects",
    "section": "",
    "text": "A Replication of Karlan and List (2007)\n\n\n\n\n\n\nDingran Wang\n\n\nApr 21, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/project1/index.html",
    "href": "blog/project1/index.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nTo explore how different fundraising strategies influence donor behavior, Karlan and List leveraged a natural field experiment in collaboration with a real nonprofit organization. The 50,000 prior donors were randomly assigned to receive one of several versions of a direct mail solicitation. The control group received a standard appeal, while treatment groups received letters offering matching grants at different match ratios—1:1, 2:1, or 3:1. Each variation also manipulated the suggested donation amount and the maximum size of the matching gift. By analyzing the response rate and donation amounts across treatments, the authors were able to isolate the causal impact of these fundraising tactics on charitable giving.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "blog/project1/index.html#section-1-data",
    "href": "blog/project1/index.html#section-1-data",
    "title": "This is Project 1",
    "section": "",
    "text": "I cleaned some data"
  },
  {
    "objectID": "blog/project1/index.html#section-2-analysis",
    "href": "blog/project1/index.html#section-2-analysis",
    "title": "This is Project 1",
    "section": "",
    "text": "I analyzed the data"
  },
  {
    "objectID": "blog/project1/index.html#introduction",
    "href": "blog/project1/index.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nTo explore how different fundraising strategies influence donor behavior, Karlan and List leveraged a natural field experiment in collaboration with a real nonprofit organization. The 50,000 prior donors were randomly assigned to receive one of several versions of a direct mail solicitation. The control group received a standard appeal, while treatment groups received letters offering matching grants at different match ratios—1:1, 2:1, or 3:1. Each variation also manipulated the suggested donation amount and the maximum size of the matching gift. By analyzing the response rate and donation amounts across treatments, the authors were able to isolate the causal impact of these fundraising tactics on charitable giving.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "blog/project1/index.html#data",
    "href": "blog/project1/index.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\n\nimport statsmodels.api as sm\nfrom scipy import stats\n\n# Load data\ndf['treat'] = df['treatment'].fillna(0)\n\n# Balance test function\ndef balance_test(var):\n    df_sub = df[['treat', var]].dropna()\n    treat_group = df_sub[df_sub['treat'] == 1][var]\n    control_group = df_sub[df_sub['treat'] == 0][var]\n    \n    # T-test\n    t_stat, p_val = stats.ttest_ind(treat_group, control_group, equal_var=True)\n    mean_diff = treat_group.mean() - control_group.mean()\n\n    print(f\"\\n==== Balance Test for '{var}' ====\")\n    print(f\"T-test:\")\n    print(f\"  Mean (Treatment): {treat_group.mean():.3f}\")\n    print(f\"  Mean (Control):   {control_group.mean():.3f}\")\n    print(f\"  Difference:       {mean_diff:.3f}\")\n    print(f\"  t-statistic:      {t_stat:.3f}\")\n    print(f\"  p-value:          {p_val:.4f}\")\n\n    # OLS\n    X = sm.add_constant(df_sub['treat'])\n    model = sm.OLS(df_sub[var], X).fit()\n    print(\"\\nOLS Regression:\")\n    print(model.summary().tables[1])\n\n# Run tests\nfor var in ['mrm2', 'freq', 'hpa']:\n    balance_test(var)\n\n\n==== Balance Test for 'mrm2' ====\nT-test:\n  Mean (Treatment): 13.012\n  Mean (Control):   12.998\n  Difference:       0.014\n  t-statistic:      0.119\n  p-value:          0.9049\n\nOLS Regression:\n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         12.9981      0.094    138.979      0.000      12.815      13.181\ntreat          0.0137      0.115      0.119      0.905      -0.211       0.238\n==============================================================================\n\n==== Balance Test for 'freq' ====\nT-test:\n  Mean (Treatment): 8.035\n  Mean (Control):   8.047\n  Difference:       -0.012\n  t-statistic:      -0.111\n  p-value:          0.9117\n\nOLS Regression:\n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          8.0473      0.088     91.231      0.000       7.874       8.220\ntreat         -0.0120      0.108     -0.111      0.912      -0.224       0.200\n==============================================================================\n\n==== Balance Test for 'hpa' ====\nT-test:\n  Mean (Treatment): 59.597\n  Mean (Control):   58.960\n  Difference:       0.637\n  t-statistic:      0.944\n  p-value:          0.3451\n\nOLS Regression:\n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         58.9602      0.551    107.005      0.000      57.880      60.040\ntreat          0.6371      0.675      0.944      0.345      -0.685       1.960\n=============================================================================="
  },
  {
    "objectID": "blog/project1/index.html#experimental-results",
    "href": "blog/project1/index.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\nIn all cases, the null hypothesis of equal means cannot be rejected, and both t-tests and OLS regressions yield identical conclusions. The treatment assignment is statistically uncorrelated with these pre-treatment variables, providing strong evidence that the randomization mechanism worked as intended.\nWhy this matters: Table 1 in Karlan & List (2007) serves the same purpose—showing that groups were well-balanced at baseline. This is essential for internal validity: it ensures that any post-treatment differences in giving behavior can be credibly attributed to the treatment itself, not to pre-existing differences between donors.\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\n\n\n\n\n\n\n\n\n\nThe barplot below shows that 2.20% of individuals in the treatment group donated, compared to 1.79% in the control group.\n\n# T-test\ngave_treat = df[df['treat'] == 1]['gave']\ngave_control = df[df['treat'] == 0]['gave']\nt_stat, p_val = stats.ttest_ind(gave_treat, gave_control, equal_var=True)\n\nprint(\"T-test results:\")\nprint(f\"  Mean (Treatment): {gave_treat.mean():.4f}\")\nprint(f\"  Mean (Control):   {gave_control.mean():.4f}\")\nprint(f\"  Difference:       {gave_treat.mean() - gave_control.mean():.4f}\")\nprint(f\"  t-statistic:      {t_stat:.3f}\")\nprint(f\"  p-value:          {p_val:.4f}\")\n\n# OLS regression\nX = sm.add_constant(df['treat'])\nols_model = sm.OLS(df['gave'], X).fit()\nprint(\"\\nOLS Regression:\")\nprint(ols_model.summary().tables[1])\n\nT-test results:\n  Mean (Treatment): 0.0220\n  Mean (Control):   0.0179\n  Difference:       0.0042\n  t-statistic:      3.101\n  p-value:          0.0019\n\nOLS Regression:\n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.0179      0.001     16.225      0.000       0.016       0.020\ntreat          0.0042      0.001      3.101      0.002       0.002       0.007\n==============================================================================\n\n\nThe t-test and regression agree: the difference is statistically significant at the 1% level. Offering a matching grant increased the response rate by about 0.42 percentage points, which is approximately a 22% increase relative to the control group’s donation rate.\n\n# Probit model\nprobit_model = sm.Probit(df['gave'], X).fit()\nprint(probit_model.summary())\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n                          Probit Regression Results                           \n==============================================================================\nDep. Variable:                   gave   No. Observations:                50083\nModel:                         Probit   Df Residuals:                    50081\nMethod:                           MLE   Df Model:                            1\nDate:                Mon, 21 Apr 2025   Pseudo R-squ.:               0.0009783\nTime:                        13:34:48   Log-Likelihood:                -5030.5\nconverged:                       True   LL-Null:                       -5035.4\nCovariance Type:            nonrobust   LLR p-value:                  0.001696\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -2.1001      0.023    -90.073      0.000      -2.146      -2.054\ntreat          0.0868      0.028      3.113      0.002       0.032       0.141\n==============================================================================\n\n\nThis replicates Table 3, Column 1 in the original paper, confirming that the treatment has a positive and statistically significant effect on the probability of donating.\nThese results show that simply informing potential donors that their gift would be matched made them more likely to give. Even though the absolute increase in donation rate is small, the relative effect is large and meaningful for fundraisers.\nThis supports the idea that donors are motivated not only by altruism, but also by how effective or impactful their contribution feels. The matching grant may act as a psychological signal that “now is a good time to give” or that their donation is more valuable than usual.\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n\n# treatment\ntreat_df = df[df['treat'] == 1].copy()\n\n# 1:1 vs 2:1\ngave_1 = treat_df[treat_df['ratio'] == 1]['gave']\ngave_2 = treat_df[treat_df['ratio'] == 2]['gave']\nt12, p12 = stats.ttest_ind(gave_1, gave_2)\n\n# 2:1 vs 3:1\ngave_3 = treat_df[treat_df['ratio'] == 3]['gave']\nt23, p23 = stats.ttest_ind(gave_2, gave_3)\n\nprint(f\"1:1 vs 2:1 match rate — p = {p12:.4f}\")\nprint(f\"2:1 vs 3:1 match rate — p = {p23:.4f}\")\nprint(f\"Means: 1:1 = {gave_1.mean():.4f}, 2:1 = {gave_2.mean():.4f}, 3:1 = {gave_3.mean():.4f}\")\n\n1:1 vs 2:1 match rate — p = 0.3345\n2:1 vs 3:1 match rate — p = 0.9600\nMeans: 1:1 = 0.0207, 2:1 = 0.0226, 3:1 = 0.0227\n\n\nThe results show no statistically significant differences in donation rates between the match levels. While there is a slight increase in the mean from 1:1 to 2:1 and 3:1, the p-values (0.33 and 0.96) confirm that these differences are not distinguishable from zero at conventional significance levels.\n\n# dummy\ntreat_df['ratio1'] = (treat_df['ratio'] == 1).astype(int)\ntreat_df['ratio2'] = (treat_df['ratio'] == 2).astype(int)\ntreat_df['ratio3'] = (treat_df['ratio'] == 3).astype(int)\n\n# ratio1\nX = sm.add_constant(treat_df[['ratio2', 'ratio3']])\ny = treat_df['gave']\nratio_model = sm.OLS(y, X).fit()\nprint(ratio_model.summary().tables[1])\n\n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.0207      0.001     14.912      0.000       0.018       0.023\nratio2         0.0019      0.002      0.958      0.338      -0.002       0.006\nratio3         0.0020      0.002      1.008      0.313      -0.002       0.006\n==============================================================================\n\n\nThis regression supports the t-test findings. Neither the 2:1 nor 3:1 match ratio has a statistically significant effect relative to the 1:1 match. This aligns closely with Karlan & List’s statement that “larger match ratios… had no additional impact” (p. 8).\n\ndiff_21 = gave_2.mean() - gave_1.mean()\ndiff_32 = gave_3.mean() - gave_2.mean()\n\n# OLS \ncoef_diff_21 = ratio_model.params['ratio2']\ncoef_diff_32 = ratio_model.params['ratio3'] - ratio_model.params['ratio2']\n\nprint(f\"Data-based diff (2:1 - 1:1): {diff_21:.4f}\")\nprint(f\"Data-based diff (3:1 - 2:1): {diff_32:.4f}\")\nprint(f\"Model-based diff (2:1 - 1:1): {coef_diff_21:.4f}\")\nprint(f\"Model-based diff (3:1 - 2:1): {coef_diff_32:.4f}\")\n\nData-based diff (2:1 - 1:1): 0.0019\nData-based diff (3:1 - 2:1): 0.0001\nModel-based diff (2:1 - 1:1): 0.0019\nModel-based diff (3:1 - 2:1): 0.0001\n\n\nBoth direct comparisons and model-based coefficient differences tell the same story: moving from 1:1 to 2:1 yields a small and statistically insignificant increase, and going from 2:1 to 3:1 yields essentially no change at all.\nThese results suggest that offering a match increases the chance of donation, but increasing the match ratio further does not enhance this effect. In other words, once the donor sees their gift will be matched, the degree of matching is not very motivating. This supports the notion that framing and salience—not just raw incentive size—drive much of charitable behavior.\nThis provides valuable insights for fundraisers: even modest matching offers (1:1) may be just as effective as more expensive ones (2:1 or 3:1) in driving participation.\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\nT-test / Regression on Full Sample\n\n# reg\nX_full = sm.add_constant(df['treat'])\ny_amount = df['amount']\nmodel_full = sm.OLS(y_amount, X_full).fit()\n\nprint(\"OLS on all individuals:\")\nprint(model_full.summary().tables[1])\n\nOLS on all individuals:\n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.8133      0.067     12.063      0.000       0.681       0.945\ntreat          0.1536      0.083      1.861      0.063      -0.008       0.315\n==============================================================================\n\n\nDependent variable: donation amount (all individuals)\nThis suggests that offering a matching grant increased the average donation amount by about $0.15. However, the result is only marginally significant (p ≈ 0.063). Since this regression includes non-donors (who gave $0), the result likely reflects the fact that more people gave at all in the treatment group.\nRegression Conditional on Donation\n\n# limit\ndf_positive = df[df['gave'] == 1]\n\nX_cond = sm.add_constant(df_positive['treat'])\ny_cond_amount = df_positive['amount']\nmodel_cond = sm.OLS(y_cond_amount, X_cond).fit()\n\nprint(\"\\nOLS on donors only (conditional on giving):\")\nprint(model_cond.summary().tables[1])\n\n\nOLS on donors only (conditional on giving):\n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         45.5403      2.423     18.792      0.000      40.785      50.296\ntreat         -1.6684      2.872     -0.581      0.561      -7.305       3.968\n==============================================================================\n\n\nAmong those who actually donated, individuals in the treatment group gave slightly less, on average, than those in the control group—but this difference is small and statistically insignificant.\nThis suggests that the treatment did not affect how much people gave, once they decided to give. The treatment influenced the extensive margin (whether to donate), but not the intensive margin (how much to donate).\nCausal note: Since we are conditioning on a post-treatment outcome (gave), the regression on donors only does not have a causal interpretation. It is, however, still descriptively valuable.\nHistograms of Donation Amounts (Among Donors)\n\n\n\n\n\n\n\n\n\nThe histograms show the distribution of donation amounts among donors only, with red dashed lines indicating group-specific average donations.\nThe treatment and control groups show similar right-skewed distributions, with most gifts clustered around $25–$75 and a few very large gifts (e.g., $250 or $400). The average donation was slightly higher in the control group, consistent with the regression results."
  },
  {
    "objectID": "blog/project1/index.html#simulation-experiment",
    "href": "blog/project1/index.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic behaves under repeated sampling, I simulate the process of comparing two independent groups with known probabilities of donating:\nControl group: donation follows a Bernoulli distribution with probability p = 0.018\nTreatment group: donation follows a Bernoulli distribution with probability p = 0.022\nI simulate the sampling and testing process many times, and observe how the t-statistic behaves under different sample sizes.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\n# Parameters\np_control = 0.018\np_treat = 0.022\nn_sims = 10000\nsample_sizes = [100, 500, 1000, 5000]\n\n# Function: simulate t-stats\ndef simulate_t_stats(n, sims=10000):\n    t_stats = []\n    for _ in range(sims):\n        control = np.random.binomial(1, p_control, size=n)\n        treat = np.random.binomial(1, p_treat, size=n)\n        t_stat, _ = stats.ttest_ind(treat, control)\n        t_stats.append(t_stat)\n    return t_stats\n\n# Run simulations\ntstat_results = {n: simulate_t_stats(n) for n in sample_sizes}\n\n\n# Plot sampling distributions\nfig, axes = plt.subplots(2, 2, figsize=(12, 8))\naxes = axes.ravel()\n\nfor i, n in enumerate(sample_sizes):\n    axes[i].hist(tstat_results[n], bins=40, density=True, color='skyblue', edgecolor='black')\n    axes[i].axvline(x=0, color='red', linestyle='--')\n    axes[i].set_title(f\"Sampling Distribution of t-stat (n = {n})\")\n    axes[i].set_xlabel(\"t-statistic\")\n    axes[i].set_ylabel(\"Density\")\n\nplt.suptitle(\"Sampling Distribution of t-statistics under Bernoulli Model\", fontsize=14)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nInterpretation When sample size is small (e.g., n = 100), the t-statistic distribution is wide and noisy — it’s hard to tell if there’s a real effect.\nAs sample size increases, the t-statistic distribution becomes narrower, and centered around the true difference (which is small, but positive).\nThis reflects the Law of Large Numbers: sample means converge to the population means.\nIt also illustrates the Central Limit Theorem: the difference in two sample means becomes approximately normally distributed as sample size grows.\nIn other words: with small samples, we might miss a real effect due to noise. But as sample size increases, our estimate of the effect and our test statistics become more stable and reliable.\n\nLaw of Large Numbers\nVisualization: Cumulative Average of Differences\n\n\n\n\n\n\n\n\n\nThe plot below shows the cumulative average of the differences in donation outcomes between simulated treatment and control groups:\nInitially, the average fluctuates wildly due to the high variance in small samples. For example, the first few differences swing far above and below the true mean.\nAs the number of samples increases, the cumulative average stabilizes and begins to hover around the true difference of 0.004, indicated by the red dashed line. This illustrates the Law of Large Numbers in practice: the more data we gather, the closer our estimate gets to the truth.\n\n\nCentral Limit Theorem\n\n# Parameters\np_c = 0.018\np_t = 0.022\nsample_sizes = [50, 200, 500, 1000]\nsim_count = 1000\n\n# Simulate average differences for each sample size\ndef simulate_avg_diffs(n, sims=1000):\n    diffs = []\n    for _ in range(sims):\n        control = np.random.binomial(1, p_c, size=n)\n        treatment = np.random.binomial(1, p_t, size=n)\n        diffs.append(np.mean(treatment) - np.mean(control))\n    return np.array(diffs)\n\n# Run simulations\nresults_clt = {n: simulate_avg_diffs(n) for n in sample_sizes}\n\n\n\n\n\n\n\n\n\n\nInterpretation At sample size = 50: The distribution is erratic, skewed, and shows wide dispersion. Zero is right in the center, meaning we likely wouldn’t detect a small treatment effect.\nAt sample size = 200: The histogram becomes more symmetric and bell-shaped, and the true effect (green line) begins to emerge away from zero.\nAt sample size = 500: The distribution becomes much smoother and narrower. Zero starts moving toward the tail, suggesting a higher chance of rejecting the null.\nAt sample size = 1000: The distribution is tightly centered near the true difference of 0.004, and zero is well into the left tail—implying we would confidently detect the effect in most samples.\nThese plots visually confirm the Central Limit Theorem: as sample size increases, the sampling distribution of the sample mean difference:\nBecomes more normal in shape\nNarrows in spread (reduced variance)\nCenters around the true mean difference"
  }
]