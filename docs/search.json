[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Dingran Wang",
    "section": "",
    "text": "Heyyy!"
  },
  {
    "objectID": "Resume.html",
    "href": "Resume.html",
    "title": "Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "My projects",
    "section": "",
    "text": "Poisson Regression Examples\n\n\n\n\n\n\nDingran Wang\n\n\nJun 5, 2025\n\n\n\n\n\n\n\n\n\n\n\nMultinomial Logit Model\n\n\n\n\n\n\nDingran Wang\n\n\nMay 18, 2025\n\n\n\n\n\n\n\n\n\n\n\nA Replication of Karlan and List (2007)\n\n\n\n\n\n\nDingran Wang\n\n\nApr 21, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/project1/index.html",
    "href": "blog/project1/index.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nTo explore how different fundraising strategies influence donor behavior, Karlan and List leveraged a natural field experiment in collaboration with a real nonprofit organization. The 50,000 prior donors were randomly assigned to receive one of several versions of a direct mail solicitation. The control group received a standard appeal, while treatment groups received letters offering matching grants at different match ratios—1:1, 2:1, or 3:1. Each variation also manipulated the suggested donation amount and the maximum size of the matching gift. By analyzing the response rate and donation amounts across treatments, the authors were able to isolate the causal impact of these fundraising tactics on charitable giving.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "blog/project1/index.html#section-1-data",
    "href": "blog/project1/index.html#section-1-data",
    "title": "This is Project 1",
    "section": "",
    "text": "I cleaned some data"
  },
  {
    "objectID": "blog/project1/index.html#section-2-analysis",
    "href": "blog/project1/index.html#section-2-analysis",
    "title": "This is Project 1",
    "section": "",
    "text": "I analyzed the data"
  },
  {
    "objectID": "blog/project1/index.html#introduction",
    "href": "blog/project1/index.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nTo explore how different fundraising strategies influence donor behavior, Karlan and List leveraged a natural field experiment in collaboration with a real nonprofit organization. The 50,000 prior donors were randomly assigned to receive one of several versions of a direct mail solicitation. The control group received a standard appeal, while treatment groups received letters offering matching grants at different match ratios—1:1, 2:1, or 3:1. Each variation also manipulated the suggested donation amount and the maximum size of the matching gift. By analyzing the response rate and donation amounts across treatments, the authors were able to isolate the causal impact of these fundraising tactics on charitable giving.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "blog/project1/index.html#data",
    "href": "blog/project1/index.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\n\nimport statsmodels.api as sm\nfrom scipy import stats\n\n# Load data\ndf['treat'] = df['treatment'].fillna(0)\n\n# Balance test function\ndef balance_test(var):\n    df_sub = df[['treat', var]].dropna()\n    treat_group = df_sub[df_sub['treat'] == 1][var]\n    control_group = df_sub[df_sub['treat'] == 0][var]\n    \n    # T-test\n    t_stat, p_val = stats.ttest_ind(treat_group, control_group, equal_var=True)\n    mean_diff = treat_group.mean() - control_group.mean()\n\n    print(f\"\\n==== Balance Test for '{var}' ====\")\n    print(f\"T-test:\")\n    print(f\"  Mean (Treatment): {treat_group.mean():.3f}\")\n    print(f\"  Mean (Control):   {control_group.mean():.3f}\")\n    print(f\"  Difference:       {mean_diff:.3f}\")\n    print(f\"  t-statistic:      {t_stat:.3f}\")\n    print(f\"  p-value:          {p_val:.4f}\")\n\n    # OLS\n    X = sm.add_constant(df_sub['treat'])\n    model = sm.OLS(df_sub[var], X).fit()\n    print(\"\\nOLS Regression:\")\n    print(model.summary().tables[1])\n\n# Run tests\nfor var in ['mrm2', 'freq', 'hpa']:\n    balance_test(var)\n\n\n==== Balance Test for 'mrm2' ====\nT-test:\n  Mean (Treatment): 13.012\n  Mean (Control):   12.998\n  Difference:       0.014\n  t-statistic:      0.119\n  p-value:          0.9049\n\nOLS Regression:\n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         12.9981      0.094    138.979      0.000      12.815      13.181\ntreat          0.0137      0.115      0.119      0.905      -0.211       0.238\n==============================================================================\n\n==== Balance Test for 'freq' ====\nT-test:\n  Mean (Treatment): 8.035\n  Mean (Control):   8.047\n  Difference:       -0.012\n  t-statistic:      -0.111\n  p-value:          0.9117\n\nOLS Regression:\n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          8.0473      0.088     91.231      0.000       7.874       8.220\ntreat         -0.0120      0.108     -0.111      0.912      -0.224       0.200\n==============================================================================\n\n==== Balance Test for 'hpa' ====\nT-test:\n  Mean (Treatment): 59.597\n  Mean (Control):   58.960\n  Difference:       0.637\n  t-statistic:      0.944\n  p-value:          0.3451\n\nOLS Regression:\n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         58.9602      0.551    107.005      0.000      57.880      60.040\ntreat          0.6371      0.675      0.944      0.345      -0.685       1.960\n=============================================================================="
  },
  {
    "objectID": "blog/project1/index.html#experimental-results",
    "href": "blog/project1/index.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\nIn all cases, the null hypothesis of equal means cannot be rejected, and both t-tests and OLS regressions yield identical conclusions. The treatment assignment is statistically uncorrelated with these pre-treatment variables, providing strong evidence that the randomization mechanism worked as intended.\nWhy this matters: Table 1 in Karlan & List (2007) serves the same purpose—showing that groups were well-balanced at baseline. This is essential for internal validity: it ensures that any post-treatment differences in giving behavior can be credibly attributed to the treatment itself, not to pre-existing differences between donors.\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\n\n\n\n\n\n\n\n\n\nThe barplot below shows that 2.20% of individuals in the treatment group donated, compared to 1.79% in the control group.\n\n# T-test\ngave_treat = df[df['treat'] == 1]['gave']\ngave_control = df[df['treat'] == 0]['gave']\nt_stat, p_val = stats.ttest_ind(gave_treat, gave_control, equal_var=True)\n\nprint(\"T-test results:\")\nprint(f\"  Mean (Treatment): {gave_treat.mean():.4f}\")\nprint(f\"  Mean (Control):   {gave_control.mean():.4f}\")\nprint(f\"  Difference:       {gave_treat.mean() - gave_control.mean():.4f}\")\nprint(f\"  t-statistic:      {t_stat:.3f}\")\nprint(f\"  p-value:          {p_val:.4f}\")\n\n# OLS regression\nX = sm.add_constant(df['treat'])\nols_model = sm.OLS(df['gave'], X).fit()\nprint(\"\\nOLS Regression:\")\nprint(ols_model.summary().tables[1])\n\nT-test results:\n  Mean (Treatment): 0.0220\n  Mean (Control):   0.0179\n  Difference:       0.0042\n  t-statistic:      3.101\n  p-value:          0.0019\n\nOLS Regression:\n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.0179      0.001     16.225      0.000       0.016       0.020\ntreat          0.0042      0.001      3.101      0.002       0.002       0.007\n==============================================================================\n\n\nThe t-test and regression agree: the difference is statistically significant at the 1% level. Offering a matching grant increased the response rate by about 0.42 percentage points, which is approximately a 22% increase relative to the control group’s donation rate.\n\n# Probit model\nprobit_model = sm.Probit(df['gave'], X).fit()\nprint(probit_model.summary())\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n                          Probit Regression Results                           \n==============================================================================\nDep. Variable:                   gave   No. Observations:                50083\nModel:                         Probit   Df Residuals:                    50081\nMethod:                           MLE   Df Model:                            1\nDate:                Mon, 21 Apr 2025   Pseudo R-squ.:               0.0009783\nTime:                        13:34:48   Log-Likelihood:                -5030.5\nconverged:                       True   LL-Null:                       -5035.4\nCovariance Type:            nonrobust   LLR p-value:                  0.001696\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -2.1001      0.023    -90.073      0.000      -2.146      -2.054\ntreat          0.0868      0.028      3.113      0.002       0.032       0.141\n==============================================================================\n\n\nThis replicates Table 3, Column 1 in the original paper, confirming that the treatment has a positive and statistically significant effect on the probability of donating.\nThese results show that simply informing potential donors that their gift would be matched made them more likely to give. Even though the absolute increase in donation rate is small, the relative effect is large and meaningful for fundraisers.\nThis supports the idea that donors are motivated not only by altruism, but also by how effective or impactful their contribution feels. The matching grant may act as a psychological signal that “now is a good time to give” or that their donation is more valuable than usual.\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n\n# treatment\ntreat_df = df[df['treat'] == 1].copy()\n\n# 1:1 vs 2:1\ngave_1 = treat_df[treat_df['ratio'] == 1]['gave']\ngave_2 = treat_df[treat_df['ratio'] == 2]['gave']\nt12, p12 = stats.ttest_ind(gave_1, gave_2)\n\n# 2:1 vs 3:1\ngave_3 = treat_df[treat_df['ratio'] == 3]['gave']\nt23, p23 = stats.ttest_ind(gave_2, gave_3)\n\nprint(f\"1:1 vs 2:1 match rate — p = {p12:.4f}\")\nprint(f\"2:1 vs 3:1 match rate — p = {p23:.4f}\")\nprint(f\"Means: 1:1 = {gave_1.mean():.4f}, 2:1 = {gave_2.mean():.4f}, 3:1 = {gave_3.mean():.4f}\")\n\n1:1 vs 2:1 match rate — p = 0.3345\n2:1 vs 3:1 match rate — p = 0.9600\nMeans: 1:1 = 0.0207, 2:1 = 0.0226, 3:1 = 0.0227\n\n\nThe results show no statistically significant differences in donation rates between the match levels. While there is a slight increase in the mean from 1:1 to 2:1 and 3:1, the p-values (0.33 and 0.96) confirm that these differences are not distinguishable from zero at conventional significance levels.\n\n# dummy\ntreat_df['ratio1'] = (treat_df['ratio'] == 1).astype(int)\ntreat_df['ratio2'] = (treat_df['ratio'] == 2).astype(int)\ntreat_df['ratio3'] = (treat_df['ratio'] == 3).astype(int)\n\n# ratio1\nX = sm.add_constant(treat_df[['ratio2', 'ratio3']])\ny = treat_df['gave']\nratio_model = sm.OLS(y, X).fit()\nprint(ratio_model.summary().tables[1])\n\n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.0207      0.001     14.912      0.000       0.018       0.023\nratio2         0.0019      0.002      0.958      0.338      -0.002       0.006\nratio3         0.0020      0.002      1.008      0.313      -0.002       0.006\n==============================================================================\n\n\nThis regression supports the t-test findings. Neither the 2:1 nor 3:1 match ratio has a statistically significant effect relative to the 1:1 match. This aligns closely with Karlan & List’s statement that “larger match ratios… had no additional impact” (p. 8).\n\ndiff_21 = gave_2.mean() - gave_1.mean()\ndiff_32 = gave_3.mean() - gave_2.mean()\n\n# OLS \ncoef_diff_21 = ratio_model.params['ratio2']\ncoef_diff_32 = ratio_model.params['ratio3'] - ratio_model.params['ratio2']\n\nprint(f\"Data-based diff (2:1 - 1:1): {diff_21:.4f}\")\nprint(f\"Data-based diff (3:1 - 2:1): {diff_32:.4f}\")\nprint(f\"Model-based diff (2:1 - 1:1): {coef_diff_21:.4f}\")\nprint(f\"Model-based diff (3:1 - 2:1): {coef_diff_32:.4f}\")\n\nData-based diff (2:1 - 1:1): 0.0019\nData-based diff (3:1 - 2:1): 0.0001\nModel-based diff (2:1 - 1:1): 0.0019\nModel-based diff (3:1 - 2:1): 0.0001\n\n\nBoth direct comparisons and model-based coefficient differences tell the same story: moving from 1:1 to 2:1 yields a small and statistically insignificant increase, and going from 2:1 to 3:1 yields essentially no change at all.\nThese results suggest that offering a match increases the chance of donation, but increasing the match ratio further does not enhance this effect. In other words, once the donor sees their gift will be matched, the degree of matching is not very motivating. This supports the notion that framing and salience—not just raw incentive size—drive much of charitable behavior.\nThis provides valuable insights for fundraisers: even modest matching offers (1:1) may be just as effective as more expensive ones (2:1 or 3:1) in driving participation.\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\nT-test / Regression on Full Sample\n\n# reg\nX_full = sm.add_constant(df['treat'])\ny_amount = df['amount']\nmodel_full = sm.OLS(y_amount, X_full).fit()\n\nprint(\"OLS on all individuals:\")\nprint(model_full.summary().tables[1])\n\nOLS on all individuals:\n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.8133      0.067     12.063      0.000       0.681       0.945\ntreat          0.1536      0.083      1.861      0.063      -0.008       0.315\n==============================================================================\n\n\nDependent variable: donation amount (all individuals)\nThis suggests that offering a matching grant increased the average donation amount by about $0.15. However, the result is only marginally significant (p ≈ 0.063). Since this regression includes non-donors (who gave $0), the result likely reflects the fact that more people gave at all in the treatment group.\nRegression Conditional on Donation\n\n# limit\ndf_positive = df[df['gave'] == 1]\n\nX_cond = sm.add_constant(df_positive['treat'])\ny_cond_amount = df_positive['amount']\nmodel_cond = sm.OLS(y_cond_amount, X_cond).fit()\n\nprint(\"\\nOLS on donors only (conditional on giving):\")\nprint(model_cond.summary().tables[1])\n\n\nOLS on donors only (conditional on giving):\n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         45.5403      2.423     18.792      0.000      40.785      50.296\ntreat         -1.6684      2.872     -0.581      0.561      -7.305       3.968\n==============================================================================\n\n\nAmong those who actually donated, individuals in the treatment group gave slightly less, on average, than those in the control group—but this difference is small and statistically insignificant.\nThis suggests that the treatment did not affect how much people gave, once they decided to give. The treatment influenced the extensive margin (whether to donate), but not the intensive margin (how much to donate).\nCausal note: Since we are conditioning on a post-treatment outcome (gave), the regression on donors only does not have a causal interpretation. It is, however, still descriptively valuable.\nHistograms of Donation Amounts (Among Donors)\n\n\n\n\n\n\n\n\n\nThe histograms show the distribution of donation amounts among donors only, with red dashed lines indicating group-specific average donations.\nThe treatment and control groups show similar right-skewed distributions, with most gifts clustered around $25–$75 and a few very large gifts (e.g., $250 or $400). The average donation was slightly higher in the control group, consistent with the regression results."
  },
  {
    "objectID": "blog/project1/index.html#simulation-experiment",
    "href": "blog/project1/index.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic behaves under repeated sampling, I simulate the process of comparing two independent groups with known probabilities of donating:\nControl group: donation follows a Bernoulli distribution with probability p = 0.018\nTreatment group: donation follows a Bernoulli distribution with probability p = 0.022\nI simulate the sampling and testing process many times, and observe how the t-statistic behaves under different sample sizes.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\n# Parameters\np_control = 0.018\np_treat = 0.022\nn_sims = 10000\nsample_sizes = [100, 500, 1000, 5000]\n\n# Function: simulate t-stats\ndef simulate_t_stats(n, sims=10000):\n    t_stats = []\n    for _ in range(sims):\n        control = np.random.binomial(1, p_control, size=n)\n        treat = np.random.binomial(1, p_treat, size=n)\n        t_stat, _ = stats.ttest_ind(treat, control)\n        t_stats.append(t_stat)\n    return t_stats\n\n# Run simulations\ntstat_results = {n: simulate_t_stats(n) for n in sample_sizes}\n\n\n# Plot sampling distributions\nfig, axes = plt.subplots(2, 2, figsize=(12, 8))\naxes = axes.ravel()\n\nfor i, n in enumerate(sample_sizes):\n    axes[i].hist(tstat_results[n], bins=40, density=True, color='skyblue', edgecolor='black')\n    axes[i].axvline(x=0, color='red', linestyle='--')\n    axes[i].set_title(f\"Sampling Distribution of t-stat (n = {n})\")\n    axes[i].set_xlabel(\"t-statistic\")\n    axes[i].set_ylabel(\"Density\")\n\nplt.suptitle(\"Sampling Distribution of t-statistics under Bernoulli Model\", fontsize=14)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nInterpretation When sample size is small (e.g., n = 100), the t-statistic distribution is wide and noisy — it’s hard to tell if there’s a real effect.\nAs sample size increases, the t-statistic distribution becomes narrower, and centered around the true difference (which is small, but positive).\nThis reflects the Law of Large Numbers: sample means converge to the population means.\nIt also illustrates the Central Limit Theorem: the difference in two sample means becomes approximately normally distributed as sample size grows.\nIn other words: with small samples, we might miss a real effect due to noise. But as sample size increases, our estimate of the effect and our test statistics become more stable and reliable.\n\nLaw of Large Numbers\nVisualization: Cumulative Average of Differences\n\n\n\n\n\n\n\n\n\nThe plot below shows the cumulative average of the differences in donation outcomes between simulated treatment and control groups:\nInitially, the average fluctuates wildly due to the high variance in small samples. For example, the first few differences swing far above and below the true mean.\nAs the number of samples increases, the cumulative average stabilizes and begins to hover around the true difference of 0.004, indicated by the red dashed line. This illustrates the Law of Large Numbers in practice: the more data we gather, the closer our estimate gets to the truth.\n\n\nCentral Limit Theorem\n\n# Parameters\np_c = 0.018\np_t = 0.022\nsample_sizes = [50, 200, 500, 1000]\nsim_count = 1000\n\n# Simulate average differences for each sample size\ndef simulate_avg_diffs(n, sims=1000):\n    diffs = []\n    for _ in range(sims):\n        control = np.random.binomial(1, p_c, size=n)\n        treatment = np.random.binomial(1, p_t, size=n)\n        diffs.append(np.mean(treatment) - np.mean(control))\n    return np.array(diffs)\n\n# Run simulations\nresults_clt = {n: simulate_avg_diffs(n) for n in sample_sizes}\n\n\n\n\n\n\n\n\n\n\nInterpretation At sample size = 50: The distribution is erratic, skewed, and shows wide dispersion. Zero is right in the center, meaning we likely wouldn’t detect a small treatment effect.\nAt sample size = 200: The histogram becomes more symmetric and bell-shaped, and the true effect (green line) begins to emerge away from zero.\nAt sample size = 500: The distribution becomes much smoother and narrower. Zero starts moving toward the tail, suggesting a higher chance of rejecting the null.\nAt sample size = 1000: The distribution is tightly centered near the true difference of 0.004, and zero is well into the left tail—implying we would confidently detect the effect in most samples.\nThese plots visually confirm the Central Limit Theorem: as sample size increases, the sampling distribution of the sample mean difference:\nBecomes more normal in shape\nNarrows in spread (reduced variance)\nCenters around the true mean difference"
  },
  {
    "objectID": "blog/project2/index.html",
    "href": "blog/project2/index.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\n\n\n\n\n\n\n\n\nMean number of patents by customer status:\n iscustomer\n0    3.473013\n1    4.133056\nName: patents, dtype: float64\n\n\nObservations Histogram: Firms using Blueprinty software tend to have more patents across most patent count ranges, especially between 2–5 patents.\nMeans: Non-customers: 3.47 patents on average Customers: 4.13 patents on average Difference: +0.66 patents for customers\nConclusion Customers of Blueprinty have, on average, more patents. This suggests a potential association, though causality cannot be confirmed without controlling for other factors like firm age and region.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n\n\n\n\n\n\n\n\n/var/folders/3r/3rw9brm96g5_0z5pk8thcr040000gn/T/ipykernel_97381/2098656923.py:14: FutureWarning:\n\n\n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n\n\n\n\n\n\n\n\n\n\nMean age by customer status:\n iscustomer\n0    26.101570\n1    26.900208\nName: age, dtype: float64\n\n\nObservations Region: (awaiting region plot — share if you’d like a summary)\nAge: Mean age: Non-customers: 26.10 years Customers: 26.90 years\nBoxplot shows similar distributions, with customers slightly older on average.\nConclusion Customer firms are marginally older than non-customers, but the difference is small. No major age imbalance is evident.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nLet ( Y_1, Y_2, , Y_n ) be independent observations such that\n( Y_i () ), for ( i = 1, , n ).\nThe likelihood function is: [ L() = _{i=1}^n = e^{-n} {{i=1}^n Y_i} {i=1}n ]\n\nimport numpy as np\nfrom scipy.special import gammaln  \n\ndef poisson_log_likelihood(lmbda, y):\n    \"\"\"\n    Computes the log-likelihood of the Poisson model.\n    \n    Parameters:\n        lmbda (float): Poisson rate parameter λ &gt; 0\n        y (array-like): Observed count data (non-negative integers)\n    \n    Returns:\n        float: Log-likelihood value\n    \"\"\"\n    y = np.asarray(y)\n    if lmbda &lt;= 0:\n        return -np.inf  # log-likelihood undefined for λ ≤ 0\n    \n    return np.sum(-lmbda + y * np.log(lmbda) - gammaln(y + 1))\n\n\n\n\n\n\n\n\n\n\nWe start with the log-likelihood function for ( n ) independent observations ( Y_1, , Y_n () ):\n[ L() = -n+ ( {i=1}^n Y_i ) - {i=1}^n (Y_i!) ]\nTaking the derivative with respect to ( ):\n[ L() = -n + _{i=1}^n Y_i ]\nSet the derivative equal to zero:\n[ -n + {i=1}^n Y_i = 0 = {i=1}^n Y_i = {Y} ]\nSo the maximum likelihood estimator is:\n[ _{} = {Y} ]\nThis result makes intuitive sense because the Poisson distribution’s mean is ( ), so the sample mean is a natural estimator.\n\n\nMLE of lambda: 3.6846662953477973\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\nX_raw = df1[[\"age\", \"region\", \"iscustomer\"]].copy()\nX_raw[\"age_squared\"] = X_raw[\"age\"] ** 2\n\nX_model = pd.get_dummies(X_raw, columns=[\"region\"], drop_first=True)\nX_model.insert(0, \"intercept\", 1)\n\nX_model = X_model.astype(float)\nY = df1[\"patents\"].values\n\n\ndef poisson_log_likelihood(beta, X, y):\n    beta = np.asarray(beta)\n    lin_pred = X @ beta\n    lambda_ = np.exp(np.clip(lin_pred, -20, 20))\n    return float(-np.sum(y * np.log(lambda_) - lambda_ - gammaln(y + 1)))\n\n\nfrom scipy.optimize import minimize\nfrom scipy.special import gammaln\n\n\nbeta_init = np.random.normal(0, 0.1, size=X_model.shape[1])\nres = minimize(poisson_log_likelihood, beta_init, args=(X_model, Y), method='BFGS')\nbeta_hat = res.x\n\ncov_matrix = res.hess_inv\nstandard_errors = np.sqrt(np.diag(cov_matrix))\n\nresults_df = pd.DataFrame({\n    \"Estimate\": beta_hat,\n    \"Std Error\": standard_errors\n}, index=X_model.columns)\n\ndisplay(results_df)\n\nprint(res.success)\nprint(res.message)\nprint(res.fun)\n\n\n\n\n\n\n\n\nEstimate\nStd Error\n\n\n\n\nintercept\n-0.509991\n0.172089\n\n\nage\n0.148706\n0.014316\n\n\niscustomer\n0.207609\n0.034997\n\n\nage_squared\n-0.002972\n0.000259\n\n\nregion_Northeast\n0.029155\n0.044931\n\n\nregion_Northwest\n-0.017578\n0.060654\n\n\nregion_South\n0.056565\n0.058302\n\n\nregion_Southwest\n0.050567\n0.049927\n\n\n\n\n\n\n\nFalse\nDesired error not necessarily achieved due to precision loss.\n3258.0721651862477\n\n\nInterpretation Intercept: Baseline log patent count is −0.51 when all predictors are 0.\nAge: Positive effect (+0.15), meaning older firms tend to have more patents.\nAge Squared: Negative (−0.003), indicating diminishing returns to age.\nIs Customer: Positive and meaningful (+0.21); Blueprinty customers have about 23% more patents on average (exp(0.21) ≈ 1.23).\nRegion Dummies: Coefficients are small and mixed, suggesting little regional effect on patent output.\n\n\nEstimated average treatment effect of Blueprinty: 0.7928415279956313\n\n\nConclusion On average, firms using Blueprinty are predicted to receive 0.79 more patents over 5 years than if they didn’t use the software, controlling for age and region. This suggests a meaningful positive effect of Blueprinty’s software on patent success."
  },
  {
    "objectID": "blog/project2/index.html#blueprinty-case-study",
    "href": "blog/project2/index.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\n\n\n\n\n\n\n\n\nMean number of patents by customer status:\n iscustomer\n0    3.473013\n1    4.133056\nName: patents, dtype: float64\n\n\nObservations Histogram: Firms using Blueprinty software tend to have more patents across most patent count ranges, especially between 2–5 patents.\nMeans: Non-customers: 3.47 patents on average Customers: 4.13 patents on average Difference: +0.66 patents for customers\nConclusion Customers of Blueprinty have, on average, more patents. This suggests a potential association, though causality cannot be confirmed without controlling for other factors like firm age and region.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n\n\n\n\n\n\n\n\n/var/folders/3r/3rw9brm96g5_0z5pk8thcr040000gn/T/ipykernel_97381/2098656923.py:14: FutureWarning:\n\n\n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n\n\n\n\n\n\n\n\n\n\nMean age by customer status:\n iscustomer\n0    26.101570\n1    26.900208\nName: age, dtype: float64\n\n\nObservations Region: (awaiting region plot — share if you’d like a summary)\nAge: Mean age: Non-customers: 26.10 years Customers: 26.90 years\nBoxplot shows similar distributions, with customers slightly older on average.\nConclusion Customer firms are marginally older than non-customers, but the difference is small. No major age imbalance is evident.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nLet ( Y_1, Y_2, , Y_n ) be independent observations such that\n( Y_i () ), for ( i = 1, , n ).\nThe likelihood function is: [ L() = _{i=1}^n = e^{-n} {{i=1}^n Y_i} {i=1}n ]\n\nimport numpy as np\nfrom scipy.special import gammaln  \n\ndef poisson_log_likelihood(lmbda, y):\n    \"\"\"\n    Computes the log-likelihood of the Poisson model.\n    \n    Parameters:\n        lmbda (float): Poisson rate parameter λ &gt; 0\n        y (array-like): Observed count data (non-negative integers)\n    \n    Returns:\n        float: Log-likelihood value\n    \"\"\"\n    y = np.asarray(y)\n    if lmbda &lt;= 0:\n        return -np.inf  # log-likelihood undefined for λ ≤ 0\n    \n    return np.sum(-lmbda + y * np.log(lmbda) - gammaln(y + 1))\n\n\n\n\n\n\n\n\n\n\nWe start with the log-likelihood function for ( n ) independent observations ( Y_1, , Y_n () ):\n[ L() = -n+ ( {i=1}^n Y_i ) - {i=1}^n (Y_i!) ]\nTaking the derivative with respect to ( ):\n[ L() = -n + _{i=1}^n Y_i ]\nSet the derivative equal to zero:\n[ -n + {i=1}^n Y_i = 0 = {i=1}^n Y_i = {Y} ]\nSo the maximum likelihood estimator is:\n[ _{} = {Y} ]\nThis result makes intuitive sense because the Poisson distribution’s mean is ( ), so the sample mean is a natural estimator.\n\n\nMLE of lambda: 3.6846662953477973\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\nX_raw = df1[[\"age\", \"region\", \"iscustomer\"]].copy()\nX_raw[\"age_squared\"] = X_raw[\"age\"] ** 2\n\nX_model = pd.get_dummies(X_raw, columns=[\"region\"], drop_first=True)\nX_model.insert(0, \"intercept\", 1)\n\nX_model = X_model.astype(float)\nY = df1[\"patents\"].values\n\n\ndef poisson_log_likelihood(beta, X, y):\n    beta = np.asarray(beta)\n    lin_pred = X @ beta\n    lambda_ = np.exp(np.clip(lin_pred, -20, 20))\n    return float(-np.sum(y * np.log(lambda_) - lambda_ - gammaln(y + 1)))\n\n\nfrom scipy.optimize import minimize\nfrom scipy.special import gammaln\n\n\nbeta_init = np.random.normal(0, 0.1, size=X_model.shape[1])\nres = minimize(poisson_log_likelihood, beta_init, args=(X_model, Y), method='BFGS')\nbeta_hat = res.x\n\ncov_matrix = res.hess_inv\nstandard_errors = np.sqrt(np.diag(cov_matrix))\n\nresults_df = pd.DataFrame({\n    \"Estimate\": beta_hat,\n    \"Std Error\": standard_errors\n}, index=X_model.columns)\n\ndisplay(results_df)\n\nprint(res.success)\nprint(res.message)\nprint(res.fun)\n\n\n\n\n\n\n\n\nEstimate\nStd Error\n\n\n\n\nintercept\n-0.509991\n0.172089\n\n\nage\n0.148706\n0.014316\n\n\niscustomer\n0.207609\n0.034997\n\n\nage_squared\n-0.002972\n0.000259\n\n\nregion_Northeast\n0.029155\n0.044931\n\n\nregion_Northwest\n-0.017578\n0.060654\n\n\nregion_South\n0.056565\n0.058302\n\n\nregion_Southwest\n0.050567\n0.049927\n\n\n\n\n\n\n\nFalse\nDesired error not necessarily achieved due to precision loss.\n3258.0721651862477\n\n\nInterpretation Intercept: Baseline log patent count is −0.51 when all predictors are 0.\nAge: Positive effect (+0.15), meaning older firms tend to have more patents.\nAge Squared: Negative (−0.003), indicating diminishing returns to age.\nIs Customer: Positive and meaningful (+0.21); Blueprinty customers have about 23% more patents on average (exp(0.21) ≈ 1.23).\nRegion Dummies: Coefficients are small and mixed, suggesting little regional effect on patent output.\n\n\nEstimated average treatment effect of Blueprinty: 0.7928415279956313\n\n\nConclusion On average, firms using Blueprinty are predicted to receive 0.79 more patents over 5 years than if they didn’t use the software, controlling for age and region. This suggests a meaningful positive effect of Blueprinty’s software on patent success."
  },
  {
    "objectID": "blog/project2/index.html#airbnb-case-study",
    "href": "blog/project2/index.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\ntodo: Assume the number of reviews is a good proxy for the number of bookings. Perform some exploratory data analysis to get a feel for the data, handle or drop observations with missing values on relevant variables, build one or more models (e.g., a poisson regression model for the number of bookings as proxied by the number of reviews), and interpret model coefficients to describe variation in the number of reviews as a function of the variables provided.\n\nimport statsmodels.api as sm\n\ndf2 = pd.read_csv(\"/Users/danielwang/Desktop/UCSD Spring/MGTA495 Marketing Analytics/Website/blog/project2/data/airbnb.csv\")\n\ndf_airbnb_clean = df2.dropna(subset=[\n    \"bathrooms\", \"bedrooms\",\n    \"review_scores_cleanliness\", \"review_scores_location\", \"review_scores_value\"\n])\n\n\ndf_model = df_airbnb_clean[[\n    \"number_of_reviews\", \"days\", \"room_type\", \"bathrooms\", \"bedrooms\", \"price\",\n    \"review_scores_cleanliness\", \"review_scores_location\", \"review_scores_value\",\n    \"instant_bookable\"\n]]\n\n\ndf_encoded = pd.get_dummies(df_model, columns=[\"room_type\", \"instant_bookable\"], drop_first=True)\n\n\nX_airbnb = df_encoded.drop(columns=\"number_of_reviews\")\nX_airbnb = sm.add_constant(X_airbnb).astype(np.float64)\ny_airbnb = df_encoded[\"number_of_reviews\"].astype(np.float64).values\n\n\nglm_airbnb = sm.GLM(y_airbnb, X_airbnb, family=sm.families.Poisson())\nglm_airbnb_results = glm_airbnb.fit()\n\n\nglm_airbnb_summary = glm_airbnb_results.summary2().tables[1]\n\n\n\ndf_model[\"predicted_reviews\"] = glm_airbnb_results.predict(X_airbnb)\ndf_model[\"residuals\"] = df_model[\"number_of_reviews\"] - df_model[\"predicted_reviews\"]\n\n\nplt.figure(figsize=(8, 5))\nsns.scatterplot(x=\"predicted_reviews\", y=\"number_of_reviews\", data=df_model, alpha=0.3)\nplt.plot([0, 600], [0, 600], '--', color=\"gray\")\nplt.xlabel(\"Predicted Number of Reviews\")\nplt.ylabel(\"Actual Number of Reviews\")\nplt.title(\"Predicted vs Actual Reviews\")\nplt.xlim(0, 600)\nplt.ylim(0, 600)\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\nglm_airbnb_summary\n\n/var/folders/3r/3rw9brm96g5_0z5pk8thcr040000gn/T/ipykernel_97381/3115416754.py:34: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n/var/folders/3r/3rw9brm96g5_0z5pk8thcr040000gn/T/ipykernel_97381/3115416754.py:35: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCoef.\nStd.Err.\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\n\n\nconst\n3.498049\n1.609066e-02\n217.396336\n0.000000e+00\n3.466512\n3.529587\n\n\ndays\n0.000051\n3.909218e-07\n129.755337\n0.000000e+00\n0.000050\n0.000051\n\n\nbathrooms\n-0.117704\n3.749225e-03\n-31.394205\n2.427557e-216\n-0.125052\n-0.110356\n\n\nbedrooms\n0.074087\n1.991742e-03\n37.197222\n7.567674e-303\n0.070184\n0.077991\n\n\nprice\n-0.000018\n8.326458e-06\n-2.150886\n3.148517e-02\n-0.000034\n-0.000002\n\n\nreview_scores_cleanliness\n0.113139\n1.496336e-03\n75.610552\n0.000000e+00\n0.110206\n0.116072\n\n\nreview_scores_location\n-0.076899\n1.608903e-03\n-47.796153\n0.000000e+00\n-0.080053\n-0.073746\n\n\nreview_scores_value\n-0.091076\n1.803855e-03\n-50.489904\n0.000000e+00\n-0.094612\n-0.087541\n\n\nroom_type_Private room\n-0.010536\n2.738448e-03\n-3.847467\n1.193451e-04\n-0.015903\n-0.005169\n\n\nroom_type_Shared room\n-0.246337\n8.619793e-03\n-28.578053\n1.259254e-179\n-0.263231\n-0.229442\n\n\ninstant_bookable_t\n0.345850\n2.890138e-03\n119.665624\n0.000000e+00\n0.340186\n0.351515\n\n\n\n\n\n\n\nThe Poisson regression shows that listings with higher cleanliness, value scores, and more bathrooms receive significantly more reviews, suggesting these factors drive bookings. In contrast, higher prices, more bedrooms, and private/shared room types are associated with fewer reviews. Interestingly, instant bookable listings receive fewer reviews, possibly reflecting more automated, less personalized experiences. The number of days listed also has a small positive effect. Overall, the model captures key drivers of booking activity, though it underpredicts high-review listings, indicating potential overdispersion."
  },
  {
    "objectID": "blog/project3/index.html",
    "href": "blog/project3/index.html",
    "title": "Multinomial Logit Model",
    "section": "",
    "text": "This assignment expores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm."
  },
  {
    "objectID": "blog/project3/index.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "href": "blog/project3/index.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "title": "Multinomial Logit Model",
    "section": "1. Likelihood for the Multi-nomial Logit (MNL) Model",
    "text": "1. Likelihood for the Multi-nomial Logit (MNL) Model\nSuppose we have \\(i=1,\\ldots,n\\) consumers who each select exactly one product \\(j\\) from a set of \\(J\\) products. The outcome variable is the identity of the product chosen \\(y_i \\in \\{1, \\ldots, J\\}\\) or equivalently a vector of \\(J-1\\) zeros and \\(1\\) one, where the \\(1\\) indicates the selected product. For example, if the third product was chosen out of 3 products, then either \\(y=3\\) or \\(y=(0,0,1)\\) depending on how we want to represent it. Suppose also that we have a vector of data on each product \\(x_j\\) (eg, brand, price, etc.).\nWe model the consumer’s decision as the selection of the product that provides the most utility, and we’ll specify the utility function as a linear function of the product characteristics:\n\\[ U_{ij} = x_j'\\beta + \\epsilon_{ij} \\]\nwhere \\(\\epsilon_{ij}\\) is an i.i.d. extreme value error term.\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer \\(i\\) chooses product \\(j\\):\n\\[ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nFor example, if there are 3 products, the probability that consumer \\(i\\) chooses product 3 is:\n\\[ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta}} \\]\nA clever way to write the individual likelihood function for consumer \\(i\\) is the product of the \\(J\\) probabilities, each raised to the power of an indicator variable (\\(\\delta_{ij}\\)) that indicates the chosen product:\n\\[ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}\\]\nNotice that if the consumer selected product \\(j=3\\), then \\(\\delta_{i3}=1\\) while \\(\\delta_{i1}=\\delta_{i2}=0\\) and the likelihood is:\n\\[ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^3e^{x_k'\\beta}} \\]\nThe joint likelihood (across all consumers) is the product of the \\(n\\) individual likelihoods:\n\\[ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} \\]\nAnd the joint log-likelihood function is:\n\\[ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) \\]"
  },
  {
    "objectID": "blog/project3/index.html#simulate-conjoint-data",
    "href": "blog/project3/index.html#simulate-conjoint-data",
    "title": "Multinomial Logit Model",
    "section": "2. Simulate Conjoint Data",
    "text": "2. Simulate Conjoint Data\nWe will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a “no choice” option; each simulated respondent must select one of the 3 alternatives.\nEach alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from $4 to $32 in increments of $4.\nThe part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer \\(i\\) for hypothethical streaming service \\(j\\) is\n\\[\nu_{ij} = (1 \\times Netflix_j) + (0.5 \\times Prime_j) + (-0.8*Ads_j) - 0.1\\times Price_j + \\varepsilon_{ij}\n\\]\nwhere the variables are binary indicators and \\(\\varepsilon\\) is Type 1 Extreme Value (ie, Gumble) distributed."
  },
  {
    "objectID": "blog/project3/index.html#preparing-the-data-for-estimation",
    "href": "blog/project3/index.html#preparing-the-data-for-estimation",
    "title": "Multinomial Logit Model",
    "section": "3. Preparing the Data for Estimation",
    "text": "3. Preparing the Data for Estimation\nThe “hard part” of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer \\(i\\), covariate \\(k\\), and product \\(j\\)) instead of the typical 2 dimensions for cross-sectional regression models (consumer \\(i\\) and covariate \\(k\\)). The fact that each task for each respondent has the same number of alternatives (3) helps. In addition, we need to convert the categorical variables for brand and ads into binary variables.\n\n\n   is_Netflix  is_Prime  has_Ads  price  choice  resp  task\n0        True     False     True     28       1     1     1\n1       False     False     True     16       0     1     1\n2       False      True     True     16       0     1     1\n3        True     False     True     32       0     1     2\n4       False      True     True     16       1     1     2"
  },
  {
    "objectID": "blog/project3/index.html#estimation-via-maximum-likelihood",
    "href": "blog/project3/index.html#estimation-via-maximum-likelihood",
    "title": "Multinomial Logit Model",
    "section": "4. Estimation via Maximum Likelihood",
    "text": "4. Estimation via Maximum Likelihood\n\nfrom scipy.optimize import minimize\nfrom scipy.special import logsumexp\n\n# List of covariate column names\ncovariates = ['is_Netflix', 'is_Prime', 'has_Ads', 'price']\n\n# 1) Force conversion of covariates to numeric and build design matrix\nX_numeric = X[covariates].apply(pd.to_numeric, errors='raise')\nX_mat     = X_numeric.to_numpy(dtype=float)\n\n# 2) Convert choice indicator to numeric array\ny = pd.to_numeric(X['choice'], errors='raise').to_numpy(dtype=float)\n\n# 3) Create group identifiers for each (resp, task) pair\ngroups   = (\n    X\n    .groupby(['resp', 'task'])\n    .ngroup()\n    .astype(int)\n    .to_numpy()\n)\nn_groups = groups.max() + 1\nK        = len(covariates)\n\n# Define negative log-likelihood for the MNL model\ndef negative_log_likelihood(beta):\n    xb     = X_mat.dot(beta)               # linear predictor\n    logden = np.zeros_like(xb)\n    for g in range(n_groups):\n        mask = (groups == g)\n        logden[mask] = logsumexp(xb[mask])\n    log_prob = xb - logden\n    return - (y * log_prob).sum()\n\n# Initial parameter guess\nbeta_initial = np.zeros(K)\n\n# Optimize to find MLE\nresult = minimize(\n    negative_log_likelihood,\n    beta_initial,\n    method='BFGS',\n    options={'disp': True}\n)\n\nbeta_hat = result.x\ncov_beta = result.hess_inv            # approximate covariance matrix\nse_beta  = np.sqrt(np.diag(cov_beta)) \n\n# Compute 95% confidence intervals\nz       = 1.96\nci_lower = beta_hat - z * se_beta\nci_upper = beta_hat + z * se_beta\n\n\nprint(\"Parameter estimates (MLE), Standard Errors, 95% CI\")\nfor name, est, se, lo, up in zip(covariates, beta_hat, se_beta, ci_lower, ci_upper):\n    print(f\"{name:&gt;12s}: {est:8.4f}  {se:8.4f}  ({lo:8.4f}, {up:8.4f})\")\n\nOptimization terminated successfully.\n         Current function value: 879.855368\n         Iterations: 12\n         Function evaluations: 85\n         Gradient evaluations: 17\nParameter estimates (MLE), Standard Errors, 95% CI\n  is_Netflix:   0.9412    0.1146  (  0.7166,   1.1658)\n    is_Prime:   0.5016    0.1207  (  0.2650,   0.7383)\n     has_Ads:  -0.7320    0.0886  ( -0.9057,  -0.5583)\n       price:  -0.0995    0.0064  ( -0.1119,  -0.0870)"
  },
  {
    "objectID": "blog/project3/index.html#estimation-via-bayesian-methods",
    "href": "blog/project3/index.html#estimation-via-bayesian-methods",
    "title": "Multinomial Logit Model",
    "section": "5. Estimation via Bayesian Methods",
    "text": "5. Estimation via Bayesian Methods\n\n# === 1) Metropolis–Hastings MCMC ===\nfrom scipy.stats import norm\n\ndef log_prior(beta):\n    lp = norm.logpdf(beta[0], 0, np.sqrt(5))\n    lp += norm.logpdf(beta[1], 0, np.sqrt(5))\n    lp += norm.logpdf(beta[2], 0, np.sqrt(5))\n    lp += norm.logpdf(beta[3], 0, 1.0)\n    return lp\n\ndef log_posterior(beta):\n    return -negative_log_likelihood(beta) + log_prior(beta)\n\nn_iter      = 11_000\nburn_in     = 1_000\nproposal_sd = np.array([0.05, 0.05, 0.05, 0.005])\n\nchain = np.zeros((n_iter, 4))\nbeta_curr = beta_initial.copy()\nlp_curr   = log_posterior(beta_curr)\nn_accept  = 0\n\nfor t in range(n_iter):\n    beta_prop = beta_curr + np.random.normal(0, proposal_sd)\n    lp_prop   = log_posterior(beta_prop)\n    if np.log(np.random.rand()) &lt; (lp_prop - lp_curr):\n        beta_curr = beta_prop\n        lp_curr   = lp_prop\n        n_accept += 1\n    chain[t] = beta_curr\n\nprint(f\"Acceptance rate: {n_accept/n_iter:.3f}\")\nsamples = chain[burn_in:]\n\nAcceptance rate: 0.573\n\n\nhint: Use N(0,5) priors for the betas on the binary variables, and a N(0,1) prior for the price beta.\n_hint: instead of calculating post=lik*prior, you can work in the log-space and calculate log-post = log-lik + log-prior (this should enable you to re-use your log-likelihood function from the MLE section just above)_\nhint: King Markov (in the video) use a candidate distribution of a coin flip to decide whether to move left or right among his islands. Unlike King Markov, we have 4 dimensions (because we have 4 betas) and our dimensions are continuous. So, use a multivariate normal distribution to pospose the next location for the algorithm to move to. I recommend a MNV(mu, Sigma) where mu=c(0,0,0,0) and sigma has diagonal values c(0.05, 0.05, 0.05, 0.005) and zeros on the off-diagonal. Since this MVN has no covariances, you can sample each dimension independently (so 4 univariate normals instead of 1 multivariate normal), where the first 3 univariate normals are N(0,0.05) and the last one if N(0,0.005).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Parameter  MLE Mean   MLE SE       MLE 95% CI  Post Mean  Post SD      Post 95% CI\nis_Netflix  0.941195 0.114606   (0.717, 1.166)   0.941829 0.114358   (0.730, 1.178)\n  is_Prime  0.501616 0.120735   (0.265, 0.738)   0.506039 0.112458   (0.298, 0.740)\n   has_Ads -0.731994 0.088639 (-0.906, -0.558)  -0.729377 0.085914 (-0.889, -0.559)\n     price -0.099480 0.006357 (-0.112, -0.087)  -0.099861 0.006330 (-0.112, -0.088)"
  },
  {
    "objectID": "blog/project3/index.html#discussion",
    "href": "blog/project3/index.html#discussion",
    "title": "Multinomial Logit Model",
    "section": "6. Discussion",
    "text": "6. Discussion\n\nInterpretation of Parameter Estimates\n\nAgreement between MLE and Bayesian estimates. The point estimates, intervals, and uncertainty from the Maximum Likelihood and Metropolis–Hastings posterior summaries are virtually identical. This indicates that with a reasonably large simulated dataset, the likelihood dominates our relatively diffuse priors, and our MCMC sampler has converged well.\nMeaning of βₙₑₜfₗᵢₓ &gt; βₚᵣᵢₘₑ. We found β Netflix ≈ 0.94 and β Prime ≈ 0.50. Holding ads and price constant, this implies a higher utility (and thus higher choice probability) for Netflix than for Amazon Prime. In other words, respondents prefer Netflix over Prime, all else equal.\nNegative price coefficient. β price ≈ –0.10 means that for each additional dollar in monthly price, utility declines by about 0.10 units. This negative sign is exactly the expected economic effect: higher price reduces consumer willingness to subscribe.\n\nExtending to a Hierarchical (Random‐Parameters) MNL Model Data simulation changes\n\nTo extend the simple MNL to a hierarchical (random‐parameters) framework, it is assumed that individual respondents exhibit heterogeneous tastes. Rather than a single fixed coefficient vector, each respondent i is assigned a personal βᵢ drawn from a population distribution N(μ, Σ). In simulation, a population mean μ and covariance Σ are first specified; for each respondent, βᵢ ∼ N(μ, Σ) is sampled. Utility then takes the form Uᵢⱼ = Xⱼ′βᵢ + εᵢⱼ, with εᵢⱼ following the extreme‐value distribution, and choices are generated according to the multinomial logit rule.\nEstimation proceeds by hierarchical Bayes: in each MCMC iteration, the full set of individual coefficients {βᵢ} is updated conditional on that respondent’s observed choices and the current hyperparameters (μ, Σ). Subsequently, μ and Σ are updated from their conditional posterior given all βᵢ draws. This alternating sampling captures both within‐respondent likelihood contributions and between‐respondent preference dispersion. Hyperpriors such as μ ∼ N(0, τ²I) and Σ ∼ Inv‐Wishart(…) complete the specification.\nImplementation requires restructuring the coefficient object from a single vector to an array of dimension (number of respondents × number of covariates). The posterior density comprises two blocks: one block of respondent‐specific terms, each involving that respondent’s choice likelihood and the prior N(μ, Σ), and one block for μ and Σ with their hyperpriors. By fitting this model, both average part‐worths and the variation and covariance of preferences across respondents are recovered."
  }
]