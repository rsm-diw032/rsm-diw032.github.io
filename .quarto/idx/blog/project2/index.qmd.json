{"title":"Poisson Regression Examples","markdown":{"yaml":{"title":"Poisson Regression Examples","author":"Dingran Wang","date":"06/05/2025","callout-appearance":"minimal"},"headingText":"Blueprinty Case Study","containsRefs":false,"markdown":"\n\n\n\n### Introduction\n\nBlueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty's software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty's software and after using it. Unfortunately, such data is not available. \n\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm's number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty's software. The marketing team would like to use this data to make the claim that firms using Blueprinty's software are more successful in getting their patent applications approved.\n\n\n### Data\n\n```{python}\n#| echo: false\nimport pandas as pd\n\ndf1 = pd.read_csv(\"/Users/danielwang/Desktop/UCSD Spring/MGTA495 Marketing Analytics/Website/blog/project2/data/blueprinty.csv\")\n\n```\n\n```{python}\n#| echo: false\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# 设置画图风格\nsns.set(style=\"whitegrid\")\n\n# 创建直方图\nplt.figure(figsize=(10, 5))\nsns.histplot(data=df1, x=\"patents\", hue=\"iscustomer\", kde=False, bins=20, palette=\"Set2\", multiple=\"dodge\")\nplt.title(\"Histogram of Patents by Customer Status\")\nplt.xlabel(\"Number of Patents\")\nplt.ylabel(\"Count\")\nplt.legend(title=\"Is Customer\", labels=[\"No\", \"Yes\"])\nplt.tight_layout()\nplt.show()\n\n# 计算均值\nmean_by_customer = df1.groupby(\"iscustomer\")[\"patents\"].mean()\nprint(\"Mean number of patents by customer status:\\n\", mean_by_customer)\n```\n\nObservations\nHistogram: Firms using Blueprinty software tend to have more patents across most patent count ranges, especially between 2–5 patents.\n\nMeans:\nNon-customers: 3.47 patents on average\nCustomers: 4.13 patents on average\nDifference: +0.66 patents for customers\n\nConclusion\nCustomers of Blueprinty have, on average, more patents. This suggests a potential association, though causality cannot be confirmed without controlling for other factors like firm age and region.\n\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n```{python}\n#| echo: false\n# Countplot for region by customer status\nplt.figure(figsize=(10, 5))\nsns.countplot(data=df1, x=\"region\", hue=\"iscustomer\", palette=\"Set2\")\nplt.title(\"Region Distribution by Customer Status\")\nplt.xlabel(\"Region\")\nplt.ylabel(\"Count\")\nplt.legend(title=\"Is Customer\", labels=[\"No\", \"Yes\"])\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\n# Boxplot for age by customer status\nplt.figure(figsize=(6, 5))\nsns.boxplot(data=df1, x=\"iscustomer\", y=\"age\", palette=\"Set2\")\nplt.title(\"Firm Age by Customer Status\")\nplt.xlabel(\"Is Customer\")\nplt.ylabel(\"Age\")\nplt.xticks([0, 1], [\"No\", \"Yes\"])\nplt.tight_layout()\nplt.show()\n\n# Mean age by customer status\nmean_age = df1.groupby(\"iscustomer\")[\"age\"].mean()\nprint(\"Mean age by customer status:\\n\", mean_age)\n\n\n```\n\nObservations\nRegion: (awaiting region plot — share if you'd like a summary)\n\nAge:\nMean age:\nNon-customers: 26.10 years\nCustomers: 26.90 years\n\nBoxplot shows similar distributions, with customers slightly older on average.\n\nConclusion\nCustomer firms are marginally older than non-customers, but the difference is small. No major age imbalance is evident.\n\n\n\n\n### Estimation of Simple Poisson Model\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\nLet \\( Y_1, Y_2, \\dots, Y_n \\) be independent observations such that  \n\\( Y_i \\sim \\text{Poisson}(\\lambda) \\), for \\( i = 1, \\dots, n \\).\n\nThe likelihood function is:\n\\[\nL(\\lambda) = \\prod_{i=1}^n \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n= e^{-n\\lambda} \\lambda^{\\sum_{i=1}^n Y_i} \\prod_{i=1}^n \\frac{1}{Y_i!}\n\\]\n\n```{python}\nimport numpy as np\nfrom scipy.special import gammaln  \n\ndef poisson_log_likelihood(lmbda, y):\n    \"\"\"\n    Computes the log-likelihood of the Poisson model.\n    \n    Parameters:\n        lmbda (float): Poisson rate parameter λ > 0\n        y (array-like): Observed count data (non-negative integers)\n    \n    Returns:\n        float: Log-likelihood value\n    \"\"\"\n    y = np.asarray(y)\n    if lmbda <= 0:\n        return -np.inf  # log-likelihood undefined for λ ≤ 0\n    \n    return np.sum(-lmbda + y * np.log(lmbda) - gammaln(y + 1))\n\n```\n\n\n\n```{python}\n#| echo: false\nlambda_values = np.linspace(0.1, 10, 200)\n\n# Compute log-likelihoods for each lambda\nlog_likelihoods = [poisson_log_likelihood(lmbda, df1[\"patents\"]) for lmbda in lambda_values]\n\n# Plot\nplt.figure(figsize=(8, 5))\nplt.plot(lambda_values, log_likelihoods, label=\"Log-Likelihood\")\nplt.xlabel(\"Lambda (λ)\")\nplt.ylabel(\"Log-Likelihood\")\nplt.title(\"Poisson Log-Likelihood vs Lambda\")\nplt.axvline(df1[\"patents\"].mean(), color=\"red\", linestyle=\"--\", label=\"Mean of Y (MLE)\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n```\n\nWe start with the log-likelihood function for \\( n \\) independent observations \\( Y_1, \\dots, Y_n \\sim \\text{Poisson}(\\lambda) \\):\n\n\\[\n\\log L(\\lambda) = -n\\lambda + \\left( \\sum_{i=1}^n Y_i \\right) \\log \\lambda - \\sum_{i=1}^n \\log(Y_i!)\n\\]\n\nTaking the derivative with respect to \\( \\lambda \\):\n\n\\[\n\\frac{d}{d\\lambda} \\log L(\\lambda) = -n + \\frac{1}{\\lambda} \\sum_{i=1}^n Y_i\n\\]\n\nSet the derivative equal to zero:\n\n\\[\n-n + \\frac{1}{\\lambda} \\sum_{i=1}^n Y_i = 0\n\\Rightarrow \\lambda = \\frac{1}{n} \\sum_{i=1}^n Y_i = \\bar{Y}\n\\]\n\nSo the maximum likelihood estimator is:\n\n\\[\n\\hat{\\lambda}_{\\text{MLE}} = \\bar{Y}\n\\]\n\nThis result makes intuitive sense because the Poisson distribution's mean is \\( \\lambda \\), so the sample mean is a natural estimator.\n\n\n```{python}\n#| echo: false\nfrom scipy.optimize import minimize\n\n# Define negative log-likelihood function (for minimization)\ndef neg_log_likelihood(lmbda):\n    return -poisson_log_likelihood(lmbda[0], df1[\"patents\"])\n\n# Initial guess\ninitial_guess = [1.0]\n\n# Optimize\nresult = minimize(neg_log_likelihood, initial_guess, bounds=[(1e-6, None)])  # λ must be > 0\n\n# Output result\nlambda_mle = result.x[0]\nprint(\"MLE of lambda:\", lambda_mle)\n\n\n```\n\n\n### Estimation of Poisson Regression Model\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that $Y_i = \\text{Poisson}(\\lambda_i)$ where $\\lambda_i = \\exp(X_i'\\beta)$. The interpretation is that the success rate of patent awards is not constant across all firms ($\\lambda$) but rather is a function of firm characteristics $X_i$. Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n```{python}\nX_raw = df1[[\"age\", \"region\", \"iscustomer\"]].copy()\nX_raw[\"age_squared\"] = X_raw[\"age\"] ** 2\n\nX_model = pd.get_dummies(X_raw, columns=[\"region\"], drop_first=True)\nX_model.insert(0, \"intercept\", 1)\n\nX_model = X_model.astype(float)\nY = df1[\"patents\"].values\n\n\ndef poisson_log_likelihood(beta, X, y):\n    beta = np.asarray(beta)\n    lin_pred = X @ beta\n    lambda_ = np.exp(np.clip(lin_pred, -20, 20))\n    return float(-np.sum(y * np.log(lambda_) - lambda_ - gammaln(y + 1)))\n\n\nfrom scipy.optimize import minimize\nfrom scipy.special import gammaln\n\n\nbeta_init = np.random.normal(0, 0.1, size=X_model.shape[1])\nres = minimize(poisson_log_likelihood, beta_init, args=(X_model, Y), method='BFGS')\nbeta_hat = res.x\n\ncov_matrix = res.hess_inv\nstandard_errors = np.sqrt(np.diag(cov_matrix))\n\nresults_df = pd.DataFrame({\n    \"Estimate\": beta_hat,\n    \"Std Error\": standard_errors\n}, index=X_model.columns)\n\ndisplay(results_df)\n\nprint(res.success)\nprint(res.message)\nprint(res.fun)\n\n```\n\n\nInterpretation\nIntercept: Baseline log patent count is −0.51 when all predictors are 0.\n\nAge: Positive effect (+0.15), meaning older firms tend to have more patents.\n\nAge Squared: Negative (−0.003), indicating diminishing returns to age.\n\nIs Customer: Positive and meaningful (+0.21); Blueprinty customers have about 23% more patents on average (exp(0.21) ≈ 1.23).\n\nRegion Dummies: Coefficients are small and mixed, suggesting little regional effect on patent output.\n\n\n\n```{python}\n#| echo: false\n# Step 1: Create counterfactual datasets\nX_0 = X_model.copy()\nX_1 = X_model.copy()\nX_0[\"iscustomer\"] = 0\nX_1[\"iscustomer\"] = 1\n\n# Step 2: Predict expected patent counts using fitted beta_hat\ny_pred_0 = np.exp(X_0.values @ beta_hat)\ny_pred_1 = np.exp(X_1.values @ beta_hat)\n\n# Step 3: Compute average treatment effect\nate = np.mean(y_pred_1 - y_pred_0)\nprint(\"Estimated average treatment effect of Blueprinty:\", ate)\n\n\n```\n\nConclusion\nOn average, firms using Blueprinty are predicted to receive 0.79 more patents over 5 years than if they didn’t use the software, controlling for age and region. This suggests a meaningful positive effect of Blueprinty's software on patent success.\n\n\n\n\n## AirBnB Case Study\n\n### Introduction\n\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City.  The data include the following variables:\n\n:::: {.callout-note collapse=\"true\"}\n### Variable Definitions\n\n    - `id` = unique ID number for each unit\n    - `last_scraped` = date when information scraped\n    - `host_since` = date when host first listed the unit on Airbnb\n    - `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n    - `room_type` = Entire home/apt., Private room, or Shared room\n    - `bathrooms` = number of bathrooms\n    - `bedrooms` = number of bedrooms\n    - `price` = price per night (dollars)\n    - `number_of_reviews` = number of reviews for the unit on Airbnb\n    - `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n    - `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n    - `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n    - `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n::::\n\n\n_todo: Assume the number of reviews is a good proxy for the number of bookings. Perform some exploratory data analysis to get a feel for the data, handle or drop observations with missing values on relevant variables, build one or more models (e.g., a poisson regression model for the number of bookings as proxied by the number of reviews), and interpret model coefficients to describe variation in the number of reviews as a function of the variables provided._\n\n\n\n```{python}\nimport statsmodels.api as sm\n\ndf2 = pd.read_csv(\"/Users/danielwang/Desktop/UCSD Spring/MGTA495 Marketing Analytics/Website/blog/project2/data/airbnb.csv\")\n\ndf_airbnb_clean = df2.dropna(subset=[\n    \"bathrooms\", \"bedrooms\",\n    \"review_scores_cleanliness\", \"review_scores_location\", \"review_scores_value\"\n])\n\n\ndf_model = df_airbnb_clean[[\n    \"number_of_reviews\", \"days\", \"room_type\", \"bathrooms\", \"bedrooms\", \"price\",\n    \"review_scores_cleanliness\", \"review_scores_location\", \"review_scores_value\",\n    \"instant_bookable\"\n]]\n\n\ndf_encoded = pd.get_dummies(df_model, columns=[\"room_type\", \"instant_bookable\"], drop_first=True)\n\n\nX_airbnb = df_encoded.drop(columns=\"number_of_reviews\")\nX_airbnb = sm.add_constant(X_airbnb).astype(np.float64)\ny_airbnb = df_encoded[\"number_of_reviews\"].astype(np.float64).values\n\n\nglm_airbnb = sm.GLM(y_airbnb, X_airbnb, family=sm.families.Poisson())\nglm_airbnb_results = glm_airbnb.fit()\n\n\nglm_airbnb_summary = glm_airbnb_results.summary2().tables[1]\n\n\n\ndf_model[\"predicted_reviews\"] = glm_airbnb_results.predict(X_airbnb)\ndf_model[\"residuals\"] = df_model[\"number_of_reviews\"] - df_model[\"predicted_reviews\"]\n\n\nplt.figure(figsize=(8, 5))\nsns.scatterplot(x=\"predicted_reviews\", y=\"number_of_reviews\", data=df_model, alpha=0.3)\nplt.plot([0, 600], [0, 600], '--', color=\"gray\")\nplt.xlabel(\"Predicted Number of Reviews\")\nplt.ylabel(\"Actual Number of Reviews\")\nplt.title(\"Predicted vs Actual Reviews\")\nplt.xlim(0, 600)\nplt.ylim(0, 600)\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\nglm_airbnb_summary\n\n\n```\n\nThe Poisson regression shows that listings with higher cleanliness, value scores, and more bathrooms receive significantly more reviews, suggesting these factors drive bookings. In contrast, higher prices, more bedrooms, and private/shared room types are associated with fewer reviews. Interestingly, instant bookable listings receive fewer reviews, possibly reflecting more automated, less personalized experiences. The number of days listed also has a small positive effect. Overall, the model captures key drivers of booking activity, though it underpredicts high-review listings, indicating potential overdispersion.","srcMarkdownNoYaml":"\n\n\n## Blueprinty Case Study\n\n### Introduction\n\nBlueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty's software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty's software and after using it. Unfortunately, such data is not available. \n\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm's number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty's software. The marketing team would like to use this data to make the claim that firms using Blueprinty's software are more successful in getting their patent applications approved.\n\n\n### Data\n\n```{python}\n#| echo: false\nimport pandas as pd\n\ndf1 = pd.read_csv(\"/Users/danielwang/Desktop/UCSD Spring/MGTA495 Marketing Analytics/Website/blog/project2/data/blueprinty.csv\")\n\n```\n\n```{python}\n#| echo: false\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# 设置画图风格\nsns.set(style=\"whitegrid\")\n\n# 创建直方图\nplt.figure(figsize=(10, 5))\nsns.histplot(data=df1, x=\"patents\", hue=\"iscustomer\", kde=False, bins=20, palette=\"Set2\", multiple=\"dodge\")\nplt.title(\"Histogram of Patents by Customer Status\")\nplt.xlabel(\"Number of Patents\")\nplt.ylabel(\"Count\")\nplt.legend(title=\"Is Customer\", labels=[\"No\", \"Yes\"])\nplt.tight_layout()\nplt.show()\n\n# 计算均值\nmean_by_customer = df1.groupby(\"iscustomer\")[\"patents\"].mean()\nprint(\"Mean number of patents by customer status:\\n\", mean_by_customer)\n```\n\nObservations\nHistogram: Firms using Blueprinty software tend to have more patents across most patent count ranges, especially between 2–5 patents.\n\nMeans:\nNon-customers: 3.47 patents on average\nCustomers: 4.13 patents on average\nDifference: +0.66 patents for customers\n\nConclusion\nCustomers of Blueprinty have, on average, more patents. This suggests a potential association, though causality cannot be confirmed without controlling for other factors like firm age and region.\n\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n```{python}\n#| echo: false\n# Countplot for region by customer status\nplt.figure(figsize=(10, 5))\nsns.countplot(data=df1, x=\"region\", hue=\"iscustomer\", palette=\"Set2\")\nplt.title(\"Region Distribution by Customer Status\")\nplt.xlabel(\"Region\")\nplt.ylabel(\"Count\")\nplt.legend(title=\"Is Customer\", labels=[\"No\", \"Yes\"])\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\n# Boxplot for age by customer status\nplt.figure(figsize=(6, 5))\nsns.boxplot(data=df1, x=\"iscustomer\", y=\"age\", palette=\"Set2\")\nplt.title(\"Firm Age by Customer Status\")\nplt.xlabel(\"Is Customer\")\nplt.ylabel(\"Age\")\nplt.xticks([0, 1], [\"No\", \"Yes\"])\nplt.tight_layout()\nplt.show()\n\n# Mean age by customer status\nmean_age = df1.groupby(\"iscustomer\")[\"age\"].mean()\nprint(\"Mean age by customer status:\\n\", mean_age)\n\n\n```\n\nObservations\nRegion: (awaiting region plot — share if you'd like a summary)\n\nAge:\nMean age:\nNon-customers: 26.10 years\nCustomers: 26.90 years\n\nBoxplot shows similar distributions, with customers slightly older on average.\n\nConclusion\nCustomer firms are marginally older than non-customers, but the difference is small. No major age imbalance is evident.\n\n\n\n\n### Estimation of Simple Poisson Model\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\nLet \\( Y_1, Y_2, \\dots, Y_n \\) be independent observations such that  \n\\( Y_i \\sim \\text{Poisson}(\\lambda) \\), for \\( i = 1, \\dots, n \\).\n\nThe likelihood function is:\n\\[\nL(\\lambda) = \\prod_{i=1}^n \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n= e^{-n\\lambda} \\lambda^{\\sum_{i=1}^n Y_i} \\prod_{i=1}^n \\frac{1}{Y_i!}\n\\]\n\n```{python}\nimport numpy as np\nfrom scipy.special import gammaln  \n\ndef poisson_log_likelihood(lmbda, y):\n    \"\"\"\n    Computes the log-likelihood of the Poisson model.\n    \n    Parameters:\n        lmbda (float): Poisson rate parameter λ > 0\n        y (array-like): Observed count data (non-negative integers)\n    \n    Returns:\n        float: Log-likelihood value\n    \"\"\"\n    y = np.asarray(y)\n    if lmbda <= 0:\n        return -np.inf  # log-likelihood undefined for λ ≤ 0\n    \n    return np.sum(-lmbda + y * np.log(lmbda) - gammaln(y + 1))\n\n```\n\n\n\n```{python}\n#| echo: false\nlambda_values = np.linspace(0.1, 10, 200)\n\n# Compute log-likelihoods for each lambda\nlog_likelihoods = [poisson_log_likelihood(lmbda, df1[\"patents\"]) for lmbda in lambda_values]\n\n# Plot\nplt.figure(figsize=(8, 5))\nplt.plot(lambda_values, log_likelihoods, label=\"Log-Likelihood\")\nplt.xlabel(\"Lambda (λ)\")\nplt.ylabel(\"Log-Likelihood\")\nplt.title(\"Poisson Log-Likelihood vs Lambda\")\nplt.axvline(df1[\"patents\"].mean(), color=\"red\", linestyle=\"--\", label=\"Mean of Y (MLE)\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n```\n\nWe start with the log-likelihood function for \\( n \\) independent observations \\( Y_1, \\dots, Y_n \\sim \\text{Poisson}(\\lambda) \\):\n\n\\[\n\\log L(\\lambda) = -n\\lambda + \\left( \\sum_{i=1}^n Y_i \\right) \\log \\lambda - \\sum_{i=1}^n \\log(Y_i!)\n\\]\n\nTaking the derivative with respect to \\( \\lambda \\):\n\n\\[\n\\frac{d}{d\\lambda} \\log L(\\lambda) = -n + \\frac{1}{\\lambda} \\sum_{i=1}^n Y_i\n\\]\n\nSet the derivative equal to zero:\n\n\\[\n-n + \\frac{1}{\\lambda} \\sum_{i=1}^n Y_i = 0\n\\Rightarrow \\lambda = \\frac{1}{n} \\sum_{i=1}^n Y_i = \\bar{Y}\n\\]\n\nSo the maximum likelihood estimator is:\n\n\\[\n\\hat{\\lambda}_{\\text{MLE}} = \\bar{Y}\n\\]\n\nThis result makes intuitive sense because the Poisson distribution's mean is \\( \\lambda \\), so the sample mean is a natural estimator.\n\n\n```{python}\n#| echo: false\nfrom scipy.optimize import minimize\n\n# Define negative log-likelihood function (for minimization)\ndef neg_log_likelihood(lmbda):\n    return -poisson_log_likelihood(lmbda[0], df1[\"patents\"])\n\n# Initial guess\ninitial_guess = [1.0]\n\n# Optimize\nresult = minimize(neg_log_likelihood, initial_guess, bounds=[(1e-6, None)])  # λ must be > 0\n\n# Output result\nlambda_mle = result.x[0]\nprint(\"MLE of lambda:\", lambda_mle)\n\n\n```\n\n\n### Estimation of Poisson Regression Model\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that $Y_i = \\text{Poisson}(\\lambda_i)$ where $\\lambda_i = \\exp(X_i'\\beta)$. The interpretation is that the success rate of patent awards is not constant across all firms ($\\lambda$) but rather is a function of firm characteristics $X_i$. Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n```{python}\nX_raw = df1[[\"age\", \"region\", \"iscustomer\"]].copy()\nX_raw[\"age_squared\"] = X_raw[\"age\"] ** 2\n\nX_model = pd.get_dummies(X_raw, columns=[\"region\"], drop_first=True)\nX_model.insert(0, \"intercept\", 1)\n\nX_model = X_model.astype(float)\nY = df1[\"patents\"].values\n\n\ndef poisson_log_likelihood(beta, X, y):\n    beta = np.asarray(beta)\n    lin_pred = X @ beta\n    lambda_ = np.exp(np.clip(lin_pred, -20, 20))\n    return float(-np.sum(y * np.log(lambda_) - lambda_ - gammaln(y + 1)))\n\n\nfrom scipy.optimize import minimize\nfrom scipy.special import gammaln\n\n\nbeta_init = np.random.normal(0, 0.1, size=X_model.shape[1])\nres = minimize(poisson_log_likelihood, beta_init, args=(X_model, Y), method='BFGS')\nbeta_hat = res.x\n\ncov_matrix = res.hess_inv\nstandard_errors = np.sqrt(np.diag(cov_matrix))\n\nresults_df = pd.DataFrame({\n    \"Estimate\": beta_hat,\n    \"Std Error\": standard_errors\n}, index=X_model.columns)\n\ndisplay(results_df)\n\nprint(res.success)\nprint(res.message)\nprint(res.fun)\n\n```\n\n\nInterpretation\nIntercept: Baseline log patent count is −0.51 when all predictors are 0.\n\nAge: Positive effect (+0.15), meaning older firms tend to have more patents.\n\nAge Squared: Negative (−0.003), indicating diminishing returns to age.\n\nIs Customer: Positive and meaningful (+0.21); Blueprinty customers have about 23% more patents on average (exp(0.21) ≈ 1.23).\n\nRegion Dummies: Coefficients are small and mixed, suggesting little regional effect on patent output.\n\n\n\n```{python}\n#| echo: false\n# Step 1: Create counterfactual datasets\nX_0 = X_model.copy()\nX_1 = X_model.copy()\nX_0[\"iscustomer\"] = 0\nX_1[\"iscustomer\"] = 1\n\n# Step 2: Predict expected patent counts using fitted beta_hat\ny_pred_0 = np.exp(X_0.values @ beta_hat)\ny_pred_1 = np.exp(X_1.values @ beta_hat)\n\n# Step 3: Compute average treatment effect\nate = np.mean(y_pred_1 - y_pred_0)\nprint(\"Estimated average treatment effect of Blueprinty:\", ate)\n\n\n```\n\nConclusion\nOn average, firms using Blueprinty are predicted to receive 0.79 more patents over 5 years than if they didn’t use the software, controlling for age and region. This suggests a meaningful positive effect of Blueprinty's software on patent success.\n\n\n\n\n## AirBnB Case Study\n\n### Introduction\n\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City.  The data include the following variables:\n\n:::: {.callout-note collapse=\"true\"}\n### Variable Definitions\n\n    - `id` = unique ID number for each unit\n    - `last_scraped` = date when information scraped\n    - `host_since` = date when host first listed the unit on Airbnb\n    - `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n    - `room_type` = Entire home/apt., Private room, or Shared room\n    - `bathrooms` = number of bathrooms\n    - `bedrooms` = number of bedrooms\n    - `price` = price per night (dollars)\n    - `number_of_reviews` = number of reviews for the unit on Airbnb\n    - `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n    - `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n    - `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n    - `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n::::\n\n\n_todo: Assume the number of reviews is a good proxy for the number of bookings. Perform some exploratory data analysis to get a feel for the data, handle or drop observations with missing values on relevant variables, build one or more models (e.g., a poisson regression model for the number of bookings as proxied by the number of reviews), and interpret model coefficients to describe variation in the number of reviews as a function of the variables provided._\n\n\n\n```{python}\nimport statsmodels.api as sm\n\ndf2 = pd.read_csv(\"/Users/danielwang/Desktop/UCSD Spring/MGTA495 Marketing Analytics/Website/blog/project2/data/airbnb.csv\")\n\ndf_airbnb_clean = df2.dropna(subset=[\n    \"bathrooms\", \"bedrooms\",\n    \"review_scores_cleanliness\", \"review_scores_location\", \"review_scores_value\"\n])\n\n\ndf_model = df_airbnb_clean[[\n    \"number_of_reviews\", \"days\", \"room_type\", \"bathrooms\", \"bedrooms\", \"price\",\n    \"review_scores_cleanliness\", \"review_scores_location\", \"review_scores_value\",\n    \"instant_bookable\"\n]]\n\n\ndf_encoded = pd.get_dummies(df_model, columns=[\"room_type\", \"instant_bookable\"], drop_first=True)\n\n\nX_airbnb = df_encoded.drop(columns=\"number_of_reviews\")\nX_airbnb = sm.add_constant(X_airbnb).astype(np.float64)\ny_airbnb = df_encoded[\"number_of_reviews\"].astype(np.float64).values\n\n\nglm_airbnb = sm.GLM(y_airbnb, X_airbnb, family=sm.families.Poisson())\nglm_airbnb_results = glm_airbnb.fit()\n\n\nglm_airbnb_summary = glm_airbnb_results.summary2().tables[1]\n\n\n\ndf_model[\"predicted_reviews\"] = glm_airbnb_results.predict(X_airbnb)\ndf_model[\"residuals\"] = df_model[\"number_of_reviews\"] - df_model[\"predicted_reviews\"]\n\n\nplt.figure(figsize=(8, 5))\nsns.scatterplot(x=\"predicted_reviews\", y=\"number_of_reviews\", data=df_model, alpha=0.3)\nplt.plot([0, 600], [0, 600], '--', color=\"gray\")\nplt.xlabel(\"Predicted Number of Reviews\")\nplt.ylabel(\"Actual Number of Reviews\")\nplt.title(\"Predicted vs Actual Reviews\")\nplt.xlim(0, 600)\nplt.ylim(0, 600)\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\nglm_airbnb_summary\n\n\n```\n\nThe Poisson regression shows that listings with higher cleanliness, value scores, and more bathrooms receive significantly more reviews, suggesting these factors drive bookings. In contrast, higher prices, more bedrooms, and private/shared room types are associated with fewer reviews. Interestingly, instant bookable listings receive fewer reviews, possibly reflecting more automated, less personalized experiences. The number of days listed also has a small positive effect. Overall, the model captures key drivers of booking activity, though it underpredicts high-review listings, indicating potential overdispersion."},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":true,"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.23","theme":["cosmo","brand"],"title":"Poisson Regression Examples","author":"Dingran Wang","date":"06/05/2025","callout-appearance":"minimal"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}