---
title: "Poisson Regression Examples"
author: "Dingran Wang"
date: 06/05/2025
callout-appearance: minimal # this hides the blue "i" icon on .callout-notes
---


## Blueprinty Case Study

### Introduction

Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty's software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty's software and after using it. Unfortunately, such data is not available. 

However, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm's number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty's software. The marketing team would like to use this data to make the claim that firms using Blueprinty's software are more successful in getting their patent applications approved.


### Data

```{python}
#| echo: false
import pandas as pd

df1 = pd.read_csv("/Users/danielwang/Desktop/UCSD Spring/MGTA495 Marketing Analytics/Website/blog/project2/data/blueprinty.csv")

```

```{python}
#| echo: false
import matplotlib.pyplot as plt
import seaborn as sns

# 设置画图风格
sns.set(style="whitegrid")

# 创建直方图
plt.figure(figsize=(10, 5))
sns.histplot(data=df1, x="patents", hue="iscustomer", kde=False, bins=20, palette="Set2", multiple="dodge")
plt.title("Histogram of Patents by Customer Status")
plt.xlabel("Number of Patents")
plt.ylabel("Count")
plt.legend(title="Is Customer", labels=["No", "Yes"])
plt.tight_layout()
plt.show()

# 计算均值
mean_by_customer = df1.groupby("iscustomer")["patents"].mean()
print("Mean number of patents by customer status:\n", mean_by_customer)
```

Observations
Histogram: Firms using Blueprinty software tend to have more patents across most patent count ranges, especially between 2–5 patents.

Means:
Non-customers: 3.47 patents on average
Customers: 4.13 patents on average
Difference: +0.66 patents for customers

Conclusion
Customers of Blueprinty have, on average, more patents. This suggests a potential association, though causality cannot be confirmed without controlling for other factors like firm age and region.

Blueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.

```{python}
#| echo: false
# Countplot for region by customer status
plt.figure(figsize=(10, 5))
sns.countplot(data=df1, x="region", hue="iscustomer", palette="Set2")
plt.title("Region Distribution by Customer Status")
plt.xlabel("Region")
plt.ylabel("Count")
plt.legend(title="Is Customer", labels=["No", "Yes"])
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# Boxplot for age by customer status
plt.figure(figsize=(6, 5))
sns.boxplot(data=df1, x="iscustomer", y="age", palette="Set2")
plt.title("Firm Age by Customer Status")
plt.xlabel("Is Customer")
plt.ylabel("Age")
plt.xticks([0, 1], ["No", "Yes"])
plt.tight_layout()
plt.show()

# Mean age by customer status
mean_age = df1.groupby("iscustomer")["age"].mean()
print("Mean age by customer status:\n", mean_age)


```

Observations
Region: (awaiting region plot — share if you'd like a summary)

Age:
Mean age:
Non-customers: 26.10 years
Customers: 26.90 years

Boxplot shows similar distributions, with customers slightly older on average.

Conclusion
Customer firms are marginally older than non-customers, but the difference is small. No major age imbalance is evident.




### Estimation of Simple Poisson Model

Since our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.

Let \( Y_1, Y_2, \dots, Y_n \) be independent observations such that  
\( Y_i \sim \text{Poisson}(\lambda) \), for \( i = 1, \dots, n \).

The likelihood function is:
\[
L(\lambda) = \prod_{i=1}^n \frac{e^{-\lambda} \lambda^{Y_i}}{Y_i!}
= e^{-n\lambda} \lambda^{\sum_{i=1}^n Y_i} \prod_{i=1}^n \frac{1}{Y_i!}
\]

```{python}
import numpy as np
from scipy.special import gammaln  

def poisson_log_likelihood(lmbda, y):
    """
    Computes the log-likelihood of the Poisson model.
    
    Parameters:
        lmbda (float): Poisson rate parameter λ > 0
        y (array-like): Observed count data (non-negative integers)
    
    Returns:
        float: Log-likelihood value
    """
    y = np.asarray(y)
    if lmbda <= 0:
        return -np.inf  # log-likelihood undefined for λ ≤ 0
    
    return np.sum(-lmbda + y * np.log(lmbda) - gammaln(y + 1))

```



```{python}
#| echo: false
lambda_values = np.linspace(0.1, 10, 200)

# Compute log-likelihoods for each lambda
log_likelihoods = [poisson_log_likelihood(lmbda, df1["patents"]) for lmbda in lambda_values]

# Plot
plt.figure(figsize=(8, 5))
plt.plot(lambda_values, log_likelihoods, label="Log-Likelihood")
plt.xlabel("Lambda (λ)")
plt.ylabel("Log-Likelihood")
plt.title("Poisson Log-Likelihood vs Lambda")
plt.axvline(df1["patents"].mean(), color="red", linestyle="--", label="Mean of Y (MLE)")
plt.legend()
plt.tight_layout()
plt.show()

```

We start with the log-likelihood function for \( n \) independent observations \( Y_1, \dots, Y_n \sim \text{Poisson}(\lambda) \):

\[
\log L(\lambda) = -n\lambda + \left( \sum_{i=1}^n Y_i \right) \log \lambda - \sum_{i=1}^n \log(Y_i!)
\]

Taking the derivative with respect to \( \lambda \):

\[
\frac{d}{d\lambda} \log L(\lambda) = -n + \frac{1}{\lambda} \sum_{i=1}^n Y_i
\]

Set the derivative equal to zero:

\[
-n + \frac{1}{\lambda} \sum_{i=1}^n Y_i = 0
\Rightarrow \lambda = \frac{1}{n} \sum_{i=1}^n Y_i = \bar{Y}
\]

So the maximum likelihood estimator is:

\[
\hat{\lambda}_{\text{MLE}} = \bar{Y}
\]

This result makes intuitive sense because the Poisson distribution's mean is \( \lambda \), so the sample mean is a natural estimator.


```{python}
#| echo: false
from scipy.optimize import minimize

# Define negative log-likelihood function (for minimization)
def neg_log_likelihood(lmbda):
    return -poisson_log_likelihood(lmbda[0], df1["patents"])

# Initial guess
initial_guess = [1.0]

# Optimize
result = minimize(neg_log_likelihood, initial_guess, bounds=[(1e-6, None)])  # λ must be > 0

# Output result
lambda_mle = result.x[0]
print("MLE of lambda:", lambda_mle)


```


### Estimation of Poisson Regression Model

Next, we extend our simple Poisson model to a Poisson Regression Model such that $Y_i = \text{Poisson}(\lambda_i)$ where $\lambda_i = \exp(X_i'\beta)$. The interpretation is that the success rate of patent awards is not constant across all firms ($\lambda$) but rather is a function of firm characteristics $X_i$. Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.

```{python}
X_raw = df1[["age", "region", "iscustomer"]].copy()
X_raw["age_squared"] = X_raw["age"] ** 2

X_model = pd.get_dummies(X_raw, columns=["region"], drop_first=True)
X_model.insert(0, "intercept", 1)

X_model = X_model.astype(float)
Y = df1["patents"].values


def poisson_log_likelihood(beta, X, y):
    beta = np.asarray(beta)
    lin_pred = X @ beta
    lambda_ = np.exp(np.clip(lin_pred, -20, 20))
    return float(-np.sum(y * np.log(lambda_) - lambda_ - gammaln(y + 1)))


from scipy.optimize import minimize
from scipy.special import gammaln


beta_init = np.random.normal(0, 0.1, size=X_model.shape[1])
res = minimize(poisson_log_likelihood, beta_init, args=(X_model, Y), method='BFGS')
beta_hat = res.x

cov_matrix = res.hess_inv
standard_errors = np.sqrt(np.diag(cov_matrix))

results_df = pd.DataFrame({
    "Estimate": beta_hat,
    "Std Error": standard_errors
}, index=X_model.columns)

display(results_df)

print(res.success)
print(res.message)
print(res.fun)

```


Interpretation
Intercept: Baseline log patent count is −0.51 when all predictors are 0.

Age: Positive effect (+0.15), meaning older firms tend to have more patents.

Age Squared: Negative (−0.003), indicating diminishing returns to age.

Is Customer: Positive and meaningful (+0.21); Blueprinty customers have about 23% more patents on average (exp(0.21) ≈ 1.23).

Region Dummies: Coefficients are small and mixed, suggesting little regional effect on patent output.



```{python}
#| echo: false
# Step 1: Create counterfactual datasets
X_0 = X_model.copy()
X_1 = X_model.copy()
X_0["iscustomer"] = 0
X_1["iscustomer"] = 1

# Step 2: Predict expected patent counts using fitted beta_hat
y_pred_0 = np.exp(X_0.values @ beta_hat)
y_pred_1 = np.exp(X_1.values @ beta_hat)

# Step 3: Compute average treatment effect
ate = np.mean(y_pred_1 - y_pred_0)
print("Estimated average treatment effect of Blueprinty:", ate)


```

Conclusion
On average, firms using Blueprinty are predicted to receive 0.79 more patents over 5 years than if they didn’t use the software, controlling for age and region. This suggests a meaningful positive effect of Blueprinty's software on patent success.




## AirBnB Case Study

### Introduction

AirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City.  The data include the following variables:

:::: {.callout-note collapse="true"}
### Variable Definitions

    - `id` = unique ID number for each unit
    - `last_scraped` = date when information scraped
    - `host_since` = date when host first listed the unit on Airbnb
    - `days` = `last_scraped` - `host_since` = number of days the unit has been listed
    - `room_type` = Entire home/apt., Private room, or Shared room
    - `bathrooms` = number of bathrooms
    - `bedrooms` = number of bedrooms
    - `price` = price per night (dollars)
    - `number_of_reviews` = number of reviews for the unit on Airbnb
    - `review_scores_cleanliness` = a cleanliness score from reviews (1-10)
    - `review_scores_location` = a "quality of location" score from reviews (1-10)
    - `review_scores_value` = a "quality of value" score from reviews (1-10)
    - `instant_bookable` = "t" if instantly bookable, "f" if not

::::


_todo: Assume the number of reviews is a good proxy for the number of bookings. Perform some exploratory data analysis to get a feel for the data, handle or drop observations with missing values on relevant variables, build one or more models (e.g., a poisson regression model for the number of bookings as proxied by the number of reviews), and interpret model coefficients to describe variation in the number of reviews as a function of the variables provided._



```{python}
import statsmodels.api as sm

df2 = pd.read_csv("/Users/danielwang/Desktop/UCSD Spring/MGTA495 Marketing Analytics/Website/blog/project2/data/airbnb.csv")

df_airbnb_clean = df2.dropna(subset=[
    "bathrooms", "bedrooms",
    "review_scores_cleanliness", "review_scores_location", "review_scores_value"
])


df_model = df_airbnb_clean[[
    "number_of_reviews", "days", "room_type", "bathrooms", "bedrooms", "price",
    "review_scores_cleanliness", "review_scores_location", "review_scores_value",
    "instant_bookable"
]]


df_encoded = pd.get_dummies(df_model, columns=["room_type", "instant_bookable"], drop_first=True)


X_airbnb = df_encoded.drop(columns="number_of_reviews")
X_airbnb = sm.add_constant(X_airbnb).astype(np.float64)
y_airbnb = df_encoded["number_of_reviews"].astype(np.float64).values


glm_airbnb = sm.GLM(y_airbnb, X_airbnb, family=sm.families.Poisson())
glm_airbnb_results = glm_airbnb.fit()


glm_airbnb_summary = glm_airbnb_results.summary2().tables[1]



df_model["predicted_reviews"] = glm_airbnb_results.predict(X_airbnb)
df_model["residuals"] = df_model["number_of_reviews"] - df_model["predicted_reviews"]


plt.figure(figsize=(8, 5))
sns.scatterplot(x="predicted_reviews", y="number_of_reviews", data=df_model, alpha=0.3)
plt.plot([0, 600], [0, 600], '--', color="gray")
plt.xlabel("Predicted Number of Reviews")
plt.ylabel("Actual Number of Reviews")
plt.title("Predicted vs Actual Reviews")
plt.xlim(0, 600)
plt.ylim(0, 600)
plt.grid(True)
plt.tight_layout()
plt.show()

glm_airbnb_summary


```

The Poisson regression shows that listings with higher cleanliness, value scores, and more bathrooms receive significantly more reviews, suggesting these factors drive bookings. In contrast, higher prices, more bedrooms, and private/shared room types are associated with fewer reviews. Interestingly, instant bookable listings receive fewer reviews, possibly reflecting more automated, less personalized experiences. The number of days listed also has a small positive effect. Overall, the model captures key drivers of booking activity, though it underpredicts high-review listings, indicating potential overdispersion.